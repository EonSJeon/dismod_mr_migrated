{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a10dd0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b42542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonc(filepath):\n",
    "    \"\"\"Load JSONC file (JSON with comments)\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Remove single-line comments (// ...)\n",
    "    content = re.sub(r'//.*?$', '', content, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove multi-line comments (/* ... */)\n",
    "    content = re.sub(r'/\\*.*?\\*/', '', content, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove trailing commas before closing brackets/braces\n",
    "    content = re.sub(r',\\s*([}\\]])', r'\\1', content)\n",
    "    \n",
    "    return json.loads(content)\n",
    "\n",
    "def load_json(filepath):\n",
    "    \"\"\"Load regular JSON file\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_any(filepath):\n",
    "    \"\"\"Load either JSON or JSONC file based on extension\"\"\"\n",
    "    if filepath.endswith('.jsonc'):\n",
    "        return load_jsonc(filepath)\n",
    "    else:\n",
    "        return load_json(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdfc5e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amd_sim_data                   parameters keys: dict_keys(['rr', 'f', 'i', 'age_weights', 'ages', 'p', 'r', 'pf', 'X'])\n",
      "amd_sim_data_Early             parameters keys: dict_keys(['rr', 'f', 'i', 'age_weights', 'ages', 'p', 'r', 'pf', 'X'])\n",
      "amd_sim_data_Intermediate      parameters keys: dict_keys(['rr', 'f', 'i', 'age_weights', 'ages', 'p', 'r', 'pf', 'X'])\n",
      "amd_sim_data_Late-dry          parameters keys: dict_keys(['rr', 'f', 'i', 'age_weights', 'ages', 'p', 'r', 'pf', 'X'])\n",
      "amd_sim_data_Late-wet          parameters keys: dict_keys(['rr', 'f', 'i', 'age_weights', 'ages', 'p', 'r', 'pf', 'X'])\n"
     ]
    }
   ],
   "source": [
    "# TASK 1: relocate age_weights to parameters.jsonc file\n",
    "input_dir = './input_data'\n",
    "stage_dirs = ['amd_sim_data', 'amd_sim_data_Early', 'amd_sim_data_Intermediate', 'amd_sim_data_Late-dry', 'amd_sim_data_Late-wet']\n",
    "\n",
    "for stage_dir in stage_dirs:\n",
    "    parameters = load_jsonc(f'{input_dir}/{stage_dir}/parameters.jsonc')\n",
    "    print(f'{stage_dir:<30} parameters keys: {parameters.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97c4128b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows: 207\n",
      "number of unique location_id: 18\n",
      "--------------------------------\n",
      "number of nodes: 233\n",
      "number of edges: 232\n"
     ]
    }
   ],
   "source": [
    "# TASK 2: update Load based on region_id_graph\n",
    "stage_dirs = ['amd_sim_data', 'amd_sim_data_Early', 'amd_sim_data_Intermediate', 'amd_sim_data_Late-dry', 'amd_sim_data_Late-wet']\n",
    "stage = stage_dirs[2] # Intermediate\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------#\n",
    "# Read input data\n",
    "\n",
    "input_data = pd.read_csv(f'{input_dir}/{stage}/input_data.csv')\n",
    "output_template = pd.read_csv(f'{input_dir}/{stage}/output_template.csv')\n",
    "parameters = load_any(f'{input_dir}/{stage}/parameters.jsonc')    # dict. {'p': {}, 'age_weights': [], 'ages: []}\n",
    "hierarchy = load_any(f'{input_dir}/{stage}/hierarchy.json')       # dict of lists. {'nodes': [], 'edges': []}\n",
    "nodes_to_fit = load_any(f'{input_dir}/{stage}/nodes_to_fit.json') # LIST of strings\n",
    "\n",
    "print(f'number of rows: {len(input_data)}')\n",
    "print(f'number of unique location_id: {input_data[\"location_id\"].nunique()}')\n",
    "print('--------------------------------')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------#\n",
    "# Create region_graph\n",
    "nodes = hierarchy['nodes']\n",
    "name_to_id = {} # warning: this does not handle duplicate names\n",
    "id_to_name = {}\n",
    "\n",
    "region_id_graph = nx.DiGraph()\n",
    "\n",
    "for node in nodes:\n",
    "  name_to_id[node[0]] = node[1]['location_id']\n",
    "  id_to_name[node[1]['location_id']] = node[0]\n",
    "\n",
    "  # create region_id_graph\n",
    "  region_id_graph.add_node(node[1]['location_id'])\n",
    "\n",
    "  my_id = node[1]['location_id']\n",
    "  parent_id = node[1]['parent_id']\n",
    "  if my_id != parent_id: # if my_id is not the root node\n",
    "    region_id_graph.add_edge(parent_id, my_id)\n",
    "\n",
    "print(f\"number of nodes: {region_id_graph.number_of_nodes()}\") \n",
    "print(f\"number of edges: {region_id_graph.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cba1b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'United States Virgin Islands'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_name[422] # IDs are not incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "826289aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "21\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# TASK 3: define \"coords\"\n",
    "\n",
    "country_list = []\n",
    "region_list = []\n",
    "super_region_list = []\n",
    "\n",
    "for node in hierarchy['nodes']:\n",
    "  if node[1]['level'] == 3:\n",
    "    country_list.append(node[1]['location_id'])\n",
    "  elif node[1]['level'] == 2:\n",
    "    region_list.append(node[1]['location_id'])\n",
    "  elif node[1]['level'] == 1:\n",
    "    super_region_list.append(node[1]['location_id'])\n",
    "    \n",
    "coords = {\n",
    "    \"country\":      country_list,\n",
    "    \"region\":       region_list,\n",
    "    \"super_region\": super_region_list,\n",
    "}\n",
    "\n",
    "print(len(country_list))\n",
    "print(len(region_list))\n",
    "print(len(super_region_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "590211ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>location_id</th>\n",
       "      <th>stage</th>\n",
       "      <th>stage_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>sex_id</th>\n",
       "      <th>year_id</th>\n",
       "      <th>age_start</th>\n",
       "      <th>age_end</th>\n",
       "      <th>effective_sample_size</th>\n",
       "      <th>value</th>\n",
       "      <th>standard_error</th>\n",
       "      <th>x_sdi</th>\n",
       "      <th>x_tob</th>\n",
       "      <th>data_type</th>\n",
       "      <th>upper_ci</th>\n",
       "      <th>lower_ci</th>\n",
       "      <th>age_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>89</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>5</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>1990</td>\n",
       "      <td>55</td>\n",
       "      <td>64</td>\n",
       "      <td>1418.0</td>\n",
       "      <td>0.040903</td>\n",
       "      <td>0.005260</td>\n",
       "      <td>0.794612</td>\n",
       "      <td>0.434156</td>\n",
       "      <td>p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>89</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>5</td>\n",
       "      <td>Female</td>\n",
       "      <td>2</td>\n",
       "      <td>1990</td>\n",
       "      <td>55</td>\n",
       "      <td>64</td>\n",
       "      <td>1802.0</td>\n",
       "      <td>0.033851</td>\n",
       "      <td>0.004260</td>\n",
       "      <td>0.794612</td>\n",
       "      <td>0.383810</td>\n",
       "      <td>p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>89</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>5</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>1990</td>\n",
       "      <td>65</td>\n",
       "      <td>74</td>\n",
       "      <td>1382.0</td>\n",
       "      <td>0.072359</td>\n",
       "      <td>0.006969</td>\n",
       "      <td>0.794612</td>\n",
       "      <td>0.434156</td>\n",
       "      <td>p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>89</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>5</td>\n",
       "      <td>Female</td>\n",
       "      <td>2</td>\n",
       "      <td>1990</td>\n",
       "      <td>65</td>\n",
       "      <td>74</td>\n",
       "      <td>1865.0</td>\n",
       "      <td>0.036997</td>\n",
       "      <td>0.004371</td>\n",
       "      <td>0.794612</td>\n",
       "      <td>0.383810</td>\n",
       "      <td>p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>89</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>5</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>1990</td>\n",
       "      <td>75</td>\n",
       "      <td>84</td>\n",
       "      <td>796.0</td>\n",
       "      <td>0.103015</td>\n",
       "      <td>0.010774</td>\n",
       "      <td>0.794612</td>\n",
       "      <td>0.434156</td>\n",
       "      <td>p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          area  location_id         stage  stage_id     sex  sex_id  year_id   \n",
       "0  Netherlands           89  Intermediate         5    Male       1     1990  \\\n",
       "1  Netherlands           89  Intermediate         5  Female       2     1990   \n",
       "2  Netherlands           89  Intermediate         5    Male       1     1990   \n",
       "3  Netherlands           89  Intermediate         5  Female       2     1990   \n",
       "4  Netherlands           89  Intermediate         5    Male       1     1990   \n",
       "\n",
       "   age_start  age_end  effective_sample_size     value  standard_error   \n",
       "0         55       64                 1418.0  0.040903        0.005260  \\\n",
       "1         55       64                 1802.0  0.033851        0.004260   \n",
       "2         65       74                 1382.0  0.072359        0.006969   \n",
       "3         65       74                 1865.0  0.036997        0.004371   \n",
       "4         75       84                  796.0  0.103015        0.010774   \n",
       "\n",
       "      x_sdi     x_tob data_type  upper_ci  lower_ci  age_weights  \n",
       "0  0.794612  0.434156         p       NaN       NaN          NaN  \n",
       "1  0.794612  0.383810         p       NaN       NaN          NaN  \n",
       "2  0.794612  0.434156         p       NaN       NaN          NaN  \n",
       "3  0.794612  0.383810         p       NaN       NaN          NaN  \n",
       "4  0.794612  0.434156         p       NaN       NaN          NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utilizes id_to_name to print the name of the location\n",
    "def describe():\n",
    "        G = region_id_graph\n",
    "        df = input_data\n",
    "        for n in nx.dfs_postorder_nodes(G, 1):\n",
    "            cnt = df['location_id'].eq(n).sum() + sum(G.nodes[c].get('cnt', 0) for c in G.successors(n))\n",
    "            G.nodes[n]['cnt'] = int(cnt)\n",
    "            G.nodes[n]['depth'] = nx.shortest_path_length(G, 1, n)\n",
    "            \n",
    "        for n in nx.dfs_preorder_nodes(G, 1):\n",
    "            if G.nodes[n]['cnt'] > 0:\n",
    "                print('  '*G.nodes[n]['depth'] + id_to_name[n], G.nodes[n]['cnt'])\n",
    "\n",
    "# describe()\n",
    "\n",
    "def keep():\n",
    "    pass\n",
    "    # Suggestion: filter input_data during \"LOAD\"\n",
    "\n",
    "def filter_input_data_by_data_type(input_data: pd.DataFrame, data_type: str) -> pd.DataFrame:\n",
    "        if not input_data.empty:\n",
    "            return input_data[input_data['data_type'] == data_type]\n",
    "        return input_data\n",
    "\n",
    "# since our input_data only has 'p' data, this will return the same input_data\n",
    "filter_input_data_by_data_type(input_data, 'p').head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b5d9f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process.asr() | parameters\n",
    "# ------------------------------------------------------------\n",
    "data_type                = 'p'\n",
    "reference_area           = 'Global'\n",
    "reference_sex            = 'Both'\n",
    "reference_year           = 'all'\n",
    "mu_age                   = None\n",
    "mu_age_parent            = None\n",
    "sigma_age_parent         = None\n",
    "rate_type                = 'neg_binom'\n",
    "lower_bound              = None\n",
    "interpolation_method     = 'linear'\n",
    "include_covariates       = True\n",
    "zero_re                  = False\n",
    "# ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dc3ce0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ages: [ 2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19.\n",
      " 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37.\n",
      " 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54. 55.\n",
      " 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71. 72. 73.\n",
      " 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89. 90. 91.\n",
      " 92. 93. 94.] | type: <class 'numpy.ndarray'> | ages.dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# process.asr() | local variables\n",
    "# ------------------------------------------------------------\n",
    "ages = np.array(parameters['ages'], dtype=np.float64)\n",
    "data = filter_input_data_by_data_type(input_data, data_type)\n",
    "lb_data =filter_input_data_by_data_type(input_data, lower_bound) if lower_bound else None\n",
    "params_of_data_type = parameters.get(data_type, {})\n",
    "\n",
    "# check: mu_age_parent & sigma_age_parent\n",
    "#  if either mu_age_parent or sigma_age_parent is NaN, set them to None\n",
    "if (isinstance(mu_age_parent, np.ndarray) and np.isnan(mu_age_parent).any()) or \\\n",
    "    (isinstance(sigma_age_parent, np.ndarray) and np.isnan(sigma_age_parent).any()):\n",
    "\n",
    "    mu_age_parent = None\n",
    "    sigma_age_parent = None\n",
    "# ------------------------------------------------------------\n",
    "print(f'ages: {ages} | type: {type(ages)} | ages.dtype: {ages.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "449ad149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knots: [ 2. 30. 45. 60. 80. 94.] | type: <class 'numpy.ndarray'> | knots.dtype: float64\n",
      "smoothing: 0.5\n"
     ]
    }
   ],
   "source": [
    "# process.asr() | [1] Prepare spline.spline (knots, smoothing)\n",
    "# ------------------------------------------------------------\n",
    "knots = np.array(params_of_data_type.get('parameter_age_mesh', np.arange(ages[0], ages[-1] + 1, 5)), dtype=np.float64)\n",
    "\n",
    "smooth_map = {'No Prior': np.inf, 'Slightly': 0.5, 'Moderately': 0.05, 'Very': 0.005}\n",
    "smoothness_param = params_of_data_type.get('smoothness')\n",
    "\n",
    "if isinstance(smoothness_param, dict): \n",
    "    amount = smoothness_param.get('amount')\n",
    "\n",
    "    if isinstance(amount, (int, float)): # smoothness_param is dict, and amount is int or float\n",
    "        smoothing = float(amount)\n",
    "    else:                                # smoothness_param is dict, and amount may be string\n",
    "        smoothing = smooth_map.get(amount, 0.0)\n",
    "\n",
    "else:                                    # smoothness_param may be string\n",
    "    smoothing = smooth_map.get(smoothness_param, 0.0)\n",
    "# ------------------------------------------------------------\n",
    "print(f'knots: {knots} | type: {type(knots)} | knots.dtype: {knots.dtype}')\n",
    "print(f'smoothing: {smoothing}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b23f5f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_model(model, var_name=None, show_shared_data=True):\n",
    "    \"\"\"\n",
    "    Inspect a PyMC model. If var_name is None, print a summary,\n",
    "    plus any shared_data contents. Otherwise, show details about a specific variable.\n",
    "    \"\"\"\n",
    "    if var_name is None:\n",
    "        print(\"üìä Model Summary:\")\n",
    "        print(f\"  ‚Ä¢ Free RVs       : {len(model.free_RVs)} {[rv.name for rv in model.free_RVs]}\")\n",
    "        print(f\"  ‚Ä¢ Observed RVs   : {len(model.observed_RVs)} {[rv.name for rv in model.observed_RVs]}\")\n",
    "        print(f\"  ‚Ä¢ Deterministics : {len(model.deterministics)} {[rv.name for rv in model.deterministics]}\")\n",
    "        print(f\"  ‚Ä¢ Potentials     : {len(model.potentials)} {[pot.name for pot in model.potentials]}\")\n",
    "        print(f\"  ‚Ä¢ Total Named RVs: {len(model.named_vars)}\")\n",
    "\n",
    "        # --- Print shared_data contents if present ---\n",
    "        if show_shared_data:\n",
    "            if hasattr(model, \"shared_data\"):\n",
    "                sd = model.shared_data\n",
    "                if isinstance(sd, dict) and sd:\n",
    "                    print(\"\\nüîñ shared_data:\")\n",
    "                    for key, val in sd.items():\n",
    "                        if isinstance(val, np.ndarray):\n",
    "                            print(f\"  ‚Ä¢ {key:15s}: array, shape={val.shape}, dtype={val.dtype}\")\n",
    "                        else:\n",
    "                            print(f\"  ‚Ä¢ {key:15s}: {val!r}\")\n",
    "\n",
    "    else:\n",
    "        var_dict = model.named_vars\n",
    "        if var_name not in var_dict:\n",
    "            print(f\"‚ùå Variable '{var_name}' not found in model.named_vars.\")\n",
    "            return\n",
    "\n",
    "        var = var_dict[var_name]\n",
    "        print(f\"üîç Variable: {var_name}\")\n",
    "        print(f\"  ‚Ä¢ Type     : {type(var)}\")\n",
    "        print(f\"  ‚Ä¢ Shape    : {getattr(var, 'shape', None)}\")\n",
    "        print(f\"  ‚Ä¢ DType    : {getattr(var, 'dtype', None)}\")\n",
    "        print(f\"  ‚Ä¢ Owner OP : {var.owner.op if getattr(var, 'owner', None) else 'None'}\")\n",
    "\n",
    "        if hasattr(var, 'distribution'):\n",
    "            dist = var.distribution\n",
    "            print(f\"  ‚Ä¢ Distribution: {dist.__class__.__name__}\")\n",
    "            if hasattr(dist, 'dist'):\n",
    "                print(f\"    - PyMC Dist : {dist.dist.__class__.__name__}\")\n",
    "            if hasattr(dist, 'kwargs'):\n",
    "                print(\"    - Parameters:\")\n",
    "                for k, v in dist.kwargs.items():\n",
    "                    print(f\"      {k}: {v}\")\n",
    "\n",
    "        if hasattr(var, 'eval'):\n",
    "            try:\n",
    "                val = var.eval()\n",
    "                print(f\"  ‚Ä¢ Current value (eval): {val}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚Ä¢ Could not evaluate variable: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bd8a4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Model Summary:\n",
      "  ‚Ä¢ Free RVs       : 0 []\n",
      "  ‚Ä¢ Observed RVs   : 0 []\n",
      "  ‚Ä¢ Deterministics : 0 []\n",
      "  ‚Ä¢ Potentials     : 0 []\n",
      "  ‚Ä¢ Total Named RVs: 0\n",
      "\n",
      "üîñ shared_data:\n",
      "  ‚Ä¢ data_type      : 'p'\n",
      "  ‚Ä¢ ages           : array, shape=(93,), dtype=float64\n",
      "  ‚Ä¢ data           :                          area  location_id         stage  stage_id     sex   \n",
      "0                 Netherlands           89  Intermediate         5    Male  \\\n",
      "1                 Netherlands           89  Intermediate         5  Female   \n",
      "2                 Netherlands           89  Intermediate         5    Male   \n",
      "3                 Netherlands           89  Intermediate         5  Female   \n",
      "4                 Netherlands           89  Intermediate         5    Male   \n",
      "..                        ...          ...           ...       ...     ...   \n",
      "202                   Ireland           84  Intermediate         5    Both   \n",
      "203                   Germany           81  Intermediate         5    Both   \n",
      "204        Russian Federation           62  Intermediate         5    Both   \n",
      "205  United States of America          102  Intermediate         5    Both   \n",
      "206  United States of America          102  Intermediate         5    Both   \n",
      "\n",
      "     sex_id  year_id  age_start  age_end  effective_sample_size     value   \n",
      "0         1     1990         55       64                 1418.0  0.040903  \\\n",
      "1         2     1990         55       64                 1802.0  0.033851   \n",
      "2         1     1990         65       74                 1382.0  0.072359   \n",
      "3         2     1990         65       74                 1865.0  0.036997   \n",
      "4         1     1990         75       84                  796.0  0.103015   \n",
      "..      ...      ...        ...      ...                    ...       ...   \n",
      "202       3     2013         85       99                   47.0  0.170213   \n",
      "203       3     2016         50       95                 4016.0  0.086155   \n",
      "204       3     2017         85       98                  932.0  0.198498   \n",
      "205       3     2018         65       95              2175803.0  0.016000   \n",
      "206       3     2019         65       95              2445163.0  0.016000   \n",
      "\n",
      "     standard_error     x_sdi     x_tob data_type  upper_ci  lower_ci   \n",
      "0          0.005260  0.794612  0.434156         p       NaN       NaN  \\\n",
      "1          0.004260  0.794612  0.383810         p       NaN       NaN   \n",
      "2          0.006969  0.794612  0.434156         p       NaN       NaN   \n",
      "3          0.004371  0.794612  0.383810         p       NaN       NaN   \n",
      "4          0.010774  0.794612  0.434156         p       NaN       NaN   \n",
      "..              ...       ...       ...       ...       ...       ...   \n",
      "202        0.054819  0.838644  0.251643         p       NaN       NaN   \n",
      "203        0.004428  0.890896  0.268396         p       NaN       NaN   \n",
      "204        0.013065  0.795669  0.301889         p       NaN       NaN   \n",
      "205        0.000085  0.855049  0.177130         p       NaN       NaN   \n",
      "206        0.000080  0.858578  0.175695         p       NaN       NaN   \n",
      "\n",
      "     age_weights  \n",
      "0            NaN  \n",
      "1            NaN  \n",
      "2            NaN  \n",
      "3            NaN  \n",
      "4            NaN  \n",
      "..           ...  \n",
      "202          NaN  \n",
      "203          NaN  \n",
      "204          NaN  \n",
      "205          NaN  \n",
      "206          NaN  \n",
      "\n",
      "[207 rows x 18 columns]\n",
      "  ‚Ä¢ lb_data        : None\n",
      "  ‚Ä¢ knots          : array, shape=(6,), dtype=float64\n",
      "  ‚Ä¢ smoothing      : 0.5\n",
      "  ‚Ä¢ interpolation_method: 'linear'\n",
      "  ‚Ä¢ params_of_data_type: {'increasing': {'age_start': 2, 'age_end': 94}, 'decreasing': {'age_start': 0, 'age_end': 0}, 'fixed_effects': {}, 'random_effects': {}, 'level_bounds': {'lower': 0.0, 'upper': 1.0}, 'y_maximum': 1.0, 'level_value': {'age_before': 30, 'age_after': 100, 'value': 0.0}, 'parameter_age_mesh': [2, 30, 45, 60, 80, 94], 'heterogeneity': 'Very', 'smoothness': {'age_start': 2, 'amount': 'Slightly', 'age_end': 94}}\n"
     ]
    }
   ],
   "source": [
    "pm_model = pm.Model()\n",
    "\n",
    "with pm_model: \n",
    "    pm_model.shared_data = {    # NOTE: this is what used to be \"vars\" from class ModelVars\n",
    "        \"data_type\": data_type,\n",
    "        \"ages\":      ages,\n",
    "        \"data\":      data,\n",
    "        \"lb_data\":   lb_data,\n",
    "        \"knots\":     knots,\n",
    "        \"smoothing\": smoothing,\n",
    "        \"interpolation_method\": interpolation_method,\n",
    "        \"params_of_data_type\": params_of_data_type\n",
    "    }\n",
    "\n",
    "inspect_model(pm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3389bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [EXAMPLE] How to use shared_data in other functions\n",
    "# ------------------------------------------------------------\n",
    "with pm_model:\n",
    "\n",
    "    # 1) Get shared_data\n",
    "    data_type = pm_model.shared_data[\"data_type\"]\n",
    "    ages      = pm_model.shared_data[\"ages\"]\n",
    "    data      = pm_model.shared_data[\"data\"]\n",
    "    lb_data   = pm_model.shared_data[\"lb_data\"]\n",
    "    knots     = pm_model.shared_data[\"knots\"]\n",
    "    smoothing = pm_model.shared_data[\"smoothing\"]\n",
    "# ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c277825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Dev/AMD/dismod_mr_migrated/reforged_mr/model/spline.py\n",
      "printing type of gamma\n",
      "<class 'list'>\n",
      "[gamma_p_0, gamma_p_1, gamma_p_2, gamma_p_3, gamma_p_4, gamma_p_5]\n",
      "printing type of gamma_vec\n",
      "<class 'pytensor.tensor.var.TensorVariable'>\n",
      "MakeVector{dtype='float64'}.0\n",
      "printing type of exp_gamma\n",
      "<class 'pytensor.tensor.var.TensorVariable'>\n",
      "Elemwise{exp,no_inplace}.0\n",
      "printing type of W_t\n",
      "<class 'pytensor.tensor.var.TensorConstant'>\n",
      "TensorConstant{[[1.      ..        ]]}\n",
      "printing type of mu_age before pm.Deterministic\n",
      "<class 'pytensor.tensor.var.TensorVariable'>\n",
      "dot.0\n",
      "printing type of mu_age after pm.Deterministic\n",
      "<class 'pytensor.tensor.var.TensorVariable'>\n",
      "dot.0\n",
      "üìä Model Summary:\n",
      "  ‚Ä¢ Free RVs       : 6 ['gamma_p_0', 'gamma_p_1', 'gamma_p_2', 'gamma_p_3', 'gamma_p_4', 'gamma_p_5']\n",
      "  ‚Ä¢ Observed RVs   : 0 []\n",
      "  ‚Ä¢ Deterministics : 1 ['mu_age_p']\n",
      "  ‚Ä¢ Potentials     : 1 ['smooth_p']\n",
      "  ‚Ä¢ Total Named RVs: 8\n",
      "-----------------------------------------------------\n",
      "üîç Variable: gamma_p_0\n",
      "  ‚Ä¢ Type     : <class 'pytensor.tensor.var.TensorVariable'>\n",
      "  ‚Ä¢ Shape    : TensorConstant{[]}\n",
      "  ‚Ä¢ DType    : float64\n",
      "  ‚Ä¢ Owner OP : normal_rv{0, (0, 0), floatX, False}\n",
      "  ‚Ä¢ Current value (eval): -1.6502997643717423\n"
     ]
    }
   ],
   "source": [
    "# process.asr() | spline.py\n",
    "import model.spline as spline\n",
    "print(spline.__file__)\n",
    "\n",
    "with pm_model:\n",
    "    if mu_age is not None:\n",
    "        unconstrained_mu_age_tv = mu_age\n",
    "        \n",
    "    else:\n",
    "        unconstrained_mu_age_tv = spline.spline()\n",
    "        \n",
    "inspect_model(pm_model, show_shared_data=False)\n",
    "print(\"-----------------------------------------------------\")\n",
    "inspect_model(pm_model, var_name='gamma_p_0', show_shared_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b1161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Dev/AMD/dismod_mr_migrated/reforged_mr/model/priors.py\n",
      "üìä Model Summary:\n",
      "  ‚Ä¢ Free RVs       : 6 ['gamma_p_0', 'gamma_p_1', 'gamma_p_2', 'gamma_p_3', 'gamma_p_4', 'gamma_p_5']\n",
      "  ‚Ä¢ Observed RVs   : 0 []\n",
      "  ‚Ä¢ Deterministics : 2 ['mu_age_p', 'value_constrained_mu_age_p']\n",
      "  ‚Ä¢ Potentials     : 2 ['smooth_p', 'parent_similarity_p']\n",
      "  ‚Ä¢ Total Named RVs: 10\n"
     ]
    }
   ],
   "source": [
    "# process.asr() | priors.py\n",
    "import model.priors as priors\n",
    "print(priors.__file__)\n",
    "\n",
    "with pm_model:\n",
    "    constrained_mu_age_tv, unconstrained_mu_age_tv, parent_similarity_tv= \\\n",
    "        priors.level_constraints(unconstrained_mu_age_tv)\n",
    "    \n",
    "\n",
    "inspect_model(pm_model, show_shared_data=False)\n",
    "# print(\"-----------------------------------------------------\")\n",
    "# inspect_model(pm_model, var_name='gamma_p_0', show_shared_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce9b72f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process.asr() | [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71be6de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process.asr() | [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d0e8c6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BELOW IS ORIGINAL CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ed4a9693",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Level 3: country\u001b[39;00m\n\u001b[1;32m      2\u001b[0m countries \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mhier\u001b[49m[hier[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLevel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLocation ID\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc3_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLocation Name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParent ID\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparent2_id\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m     })\n\u001b[1;32m      9\u001b[0m     [[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc3_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparent2_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Level 2: region\u001b[39;00m\n\u001b[1;32m     13\u001b[0m regions \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     14\u001b[0m     hier[hier[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLevel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     [[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc2_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparent1_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     21\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hier' is not defined"
     ]
    }
   ],
   "source": [
    "# Level 3: country\n",
    "countries = (\n",
    "    hier[hier[\"Level\"] == 3]\n",
    "    .rename(columns={\n",
    "        \"Location ID\": \"loc3_id\",\n",
    "        \"Location Name\": \"country\",\n",
    "        \"Parent ID\": \"parent2_id\"\n",
    "    })\n",
    "    [[\"loc3_id\", \"country\", \"parent2_id\"]]\n",
    ")\n",
    "\n",
    "# Level 2: region\n",
    "regions = (\n",
    "    hier[hier[\"Level\"] == 2]\n",
    "    .rename(columns={\n",
    "        \"Location ID\": \"loc2_id\",\n",
    "        \"Location Name\": \"region\",\n",
    "        \"Parent ID\": \"parent1_id\"\n",
    "    })\n",
    "    [[\"loc2_id\", \"region\", \"parent1_id\"]]\n",
    ")\n",
    "\n",
    "# Level 1: super-region\n",
    "superregs = (\n",
    "    hier[hier[\"Level\"] == 1]\n",
    "    .rename(columns={\n",
    "        \"Location ID\": \"loc1_id\",\n",
    "        \"Location Name\": \"super_region\"\n",
    "    })\n",
    "    [[\"loc1_id\", \"super_region\"]]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc9e0ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>super_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>High-income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States of America</td>\n",
       "      <td>High-income North America</td>\n",
       "      <td>High-income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Israel</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>High-income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>High-income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Canada</td>\n",
       "      <td>High-income North America</td>\n",
       "      <td>High-income</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    country                     region super_region\n",
       "0                    Sweden             Western Europe  High-income\n",
       "1  United States of America  High-income North America  High-income\n",
       "3                    Israel             Western Europe  High-income\n",
       "5               Switzerland             Western Europe  High-income\n",
       "7                    Canada  High-income North America  High-income"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.1 country Ï†ïÎ≥¥ Î∂ôÏù¥Í∏∞\n",
    "df = df.merge(countries, on=\"country\", how=\"left\")\n",
    "\n",
    "# 4.2 region Ï†ïÎ≥¥ Î∂ôÏù¥Í∏∞\n",
    "df = df.merge(\n",
    "    regions,\n",
    "    left_on=\"parent2_id\",\n",
    "    right_on=\"loc2_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 4.3 super-region Ï†ïÎ≥¥ Î∂ôÏù¥Í∏∞\n",
    "df = df.merge(\n",
    "    superregs,\n",
    "    left_on=\"parent1_id\",\n",
    "    right_on=\"loc1_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# ÌôïÏù∏\n",
    "df[[\"country\",\"region\",\"super_region\"]].drop_duplicates().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6de9264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     study                   country  \\\n",
      "0   Nilsson LV et al, 1984                    Sweden   \n",
      "1     Kramer M et al, 1985  United States of America   \n",
      "2    Flament M et al, 1990  United States of America   \n",
      "3     Zohar AH et al, 1992                    Israel   \n",
      "4  Reinherz HZ et al, 1993  United States of America   \n",
      "\n",
      "                      region super_region  age_code  meth_code  prev_code  \\\n",
      "0             Western Europe  High-income         1          0          0   \n",
      "1  High-income North America  High-income         1          0          0   \n",
      "2  High-income North America  High-income         1          0          0   \n",
      "3             Western Europe  High-income         0          0          0   \n",
      "4  High-income North America  High-income         0          0          0   \n",
      "\n",
      "   prevalence_value  \n",
      "0              98.0  \n",
      "1              22.0  \n",
      "2              55.0  \n",
      "3              36.0  \n",
      "4              21.0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2) Îß§Ìïë ÎîïÏÖîÎÑàÎ¶¨\n",
    "age_map  = {\"Children\":0, \"Adult\":1, \"Both\":2}\n",
    "meth_map = {\n",
    "    \"Interview based\":0,\n",
    "    \"Non-interview\":1\n",
    "}\n",
    "prev_map = {\n",
    "    \"point prevalence\":     0,\n",
    "    \"12-month prevalence\":  1,\n",
    "    \"life-time prevalence\": 2\n",
    "}\n",
    "\n",
    "# 3) long Ìè¨Îß∑ÏúºÎ°ú Ï†ÑÌôò\n",
    "df_long = df.melt(\n",
    "    id_vars=[\n",
    "        \"study\",             \n",
    "        \"country\",\n",
    "        \"region\",\n",
    "        \"super_region\",\n",
    "        \"Methods of questionnaire\",\n",
    "        \"Age range\",\n",
    "        \"study population\"\n",
    "    ],\n",
    "    value_vars=[\n",
    "        \"point prevalence\",\n",
    "        \"12-month prevalence\",\n",
    "        \"life-time prevalence\"\n",
    "    ],\n",
    "    var_name=\"prevalence_type\",\n",
    "    value_name=\"prevalence_value\"\n",
    ").dropna(subset=[\"prevalence_value\"])\n",
    "\n",
    "# 4) ÏΩîÎìú Îß§Ìïë\n",
    "df_long[\"age_code\"]  = df_long[\"Age range\"].map(age_map)\n",
    "df_long[\"meth_code\"] = df_long[\"Methods of questionnaire\"].map(meth_map)\n",
    "df_long[\"prev_code\"] = df_long[\"prevalence_type\"].map(prev_map)\n",
    "\n",
    "# 5) ÏµúÏ¢Ö ÌôïÏù∏\n",
    "result = df_long[[\n",
    "    \"study\", \"country\", \"region\", \"super_region\",\n",
    "    \"age_code\", \"meth_code\",\n",
    "    \"prev_code\", \"prevalence_value\"\n",
    "]]\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca0029bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) ÎπÑÏú®(p_hat) Î∞è Î°úÍ∑∏ Î≥ÄÌôò\n",
    "#    prevalence_value: point/12-month/life-time ÏºÄÏù¥Ïä§ Ïàò\n",
    "#    study population: Ï†ÑÏ≤¥ ÌëúÎ≥∏ Ïàò\n",
    "\n",
    "# 6.1 p_hat Í≥ÑÏÇ∞\n",
    "df_long[\"p_hat\"] = df_long[\"prevalence_value\"] / df_long[\"study population\"]\n",
    "\n",
    "# 6.2 Î°úÍ∑∏ Î≥ÄÌôò (ÌïÑÏöî Ïãú 0 ÌöåÌîºÎ•º ÏúÑÌï¥ clamp)\n",
    "eps = 1e-6\n",
    "df_long[\"p_hat\"] = df_long[\"p_hat\"].clip(eps, 1 - eps)\n",
    "df_long[\"log_p_hat\"] = np.log(df_long[\"p_hat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe5e535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study</th>\n",
       "      <th>prevalence_type</th>\n",
       "      <th>multi_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nilsson LV et al, 1984</td>\n",
       "      <td>point prevalence</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kramer M et al, 1985</td>\n",
       "      <td>point prevalence</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flament M et al, 1990</td>\n",
       "      <td>point prevalence</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zohar AH et al, 1992</td>\n",
       "      <td>point prevalence</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reinherz HZ et al, 1993</td>\n",
       "      <td>point prevalence</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     study   prevalence_type  multi_mask\n",
       "0   Nilsson LV et al, 1984  point prevalence           0\n",
       "1     Kramer M et al, 1985  point prevalence           0\n",
       "2    Flament M et al, 1990  point prevalence           1\n",
       "3     Zohar AH et al, 1992  point prevalence           0\n",
       "4  Reinherz HZ et al, 1993  point prevalence           0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) df_long Ïù¥Ïö©: prevalence_type Îãπ Í∞úÏàò ÏÑ∏Í∏∞\n",
    "prev_count = df_long.groupby(\"study\")[\"prevalence_type\"] \\\n",
    "                    .transform(\"nunique\")\n",
    "\n",
    "# 2) 2Í∞ú Ïù¥ÏÉÅÏù¥Î©¥ 1, ÏïÑÎãàÎ©¥ 0\n",
    "df_long[\"multi_mask\"] = (prev_count >= 2).astype(int)\n",
    "\n",
    "# 3) ÌôïÏù∏\n",
    "df_long[[\"study\",\"prevalence_type\",\"multi_mask\"]].drop_duplicates().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ecf578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) hierarchy ÏóêÏÑú Ï¢åÌëú(Î¶¨Ïä§Ìä∏) ÏßÅÏ†ë Ï∂îÏ∂ú\n",
    "# level 1 ‚Üí super_region (7Í∞ú)\n",
    "super_list = (\n",
    "    hier[hier[\"Level\"] == 1]\n",
    "    [\"Location Name\"]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# level 2 ‚Üí region (21Í∞ú)\n",
    "region_list = (\n",
    "    hier[hier[\"Level\"] == 2]\n",
    "    [\"Location Name\"]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# level 3 ‚Üí country (180+Í∞ú)\n",
    "country_list = (\n",
    "    hier[hier[\"Level\"] == 3]\n",
    "    [\"Location Name\"]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# 2) coords Ï†ïÏùò (unchanged)\n",
    "coords = {\n",
    "    \"country\":      country_list,\n",
    "    \"region\":       region_list,\n",
    "    \"super_region\": super_list,\n",
    "    \"age\":          [\"Children\", \"Adult\", \"Both\"],\n",
    "    \"meth\":         [\"Interview based\", \"Non-interview\"],\n",
    "    \"prev\":         [\"point prevalence\", \"12-month prevalence\", \"life-time prevalence\"],\n",
    "    \"study\":        df_long[\"study\"].unique().tolist()\n",
    "}\n",
    "\n",
    "# 3) questionnaire method Ïª¨ÎüºÎ™Ö Î≥ÄÍ≤Ω Î∞òÏòÅ\n",
    "#    Ïù¥ÎØ∏ ÏóëÏÖÄÏóêÏÑú \"Methods of questionnaire\" ÏïÑÎûò Í∞íÏùÑ \"Interview based\" / \"Non-interview\"Î°ú Î∞îÍøî ÎëêÏÖ®Îã§Î©¥\n",
    "meth_map = {\n",
    "    \"Interview based\":    0,\n",
    "    \"Non-interview\":      1\n",
    "}\n",
    "df_long[\"meth_code\"] = df_long[\"Methods of questionnaire\"].map(meth_map)\n",
    "\n",
    "# 4) index Î∞∞Ïó¥ ÏÉùÏÑ±\n",
    "country_idx = pd.Categorical(\n",
    "    df_long[\"country\"],\n",
    "    categories=coords[\"country\"]\n",
    ").codes\n",
    "\n",
    "region_idx = pd.Categorical(\n",
    "    df_long[\"region\"],\n",
    "    categories=coords[\"region\"]\n",
    ").codes\n",
    "\n",
    "super_idx = pd.Categorical(\n",
    "    df_long[\"super_region\"],\n",
    "    categories=coords[\"super_region\"]\n",
    ").codes\n",
    "\n",
    "age_idx   = df_long[\"age_code\"].to_numpy()\n",
    "meth_idx  = df_long[\"meth_code\"].to_numpy()         \n",
    "prev_idx  = df_long[\"prev_code\"].to_numpy()\n",
    "\n",
    "study_idx = pd.Categorical(\n",
    "    df_long[\"study\"],\n",
    "    categories=coords[\"study\"]\n",
    ").codes\n",
    "\n",
    "# 5) mask / Í¥ÄÏ∏°Ïπò Î≤°ÌÑ∞ (unchanged)\n",
    "multi_mask = df_long[\"multi_mask\"].to_numpy()\n",
    "y_obs      = df_long[\"log_p_hat\"].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203c1bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa9cf9e5a5442c7ad3bebb3b8c6c0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished [100%]: Average Loss = 395.52\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [Œ≤0, Œ≤_age, Œ≤_meth, Œ≤_prev, œÉ_sr, Œ∑_sr, œÉ_r, Œ∑_r, œÉ_c, Œ∑_c, œÉ_prev_study, Œ∑_prev_study, œÉ_y]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b22a72e62f74df98ebde817413195c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 7188 seconds.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "plot_forest() got an unexpected keyword argument 'credible_interval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# ‚îÄ‚îÄ‚îÄ forest plot ÏúºÎ°ú ÍπîÎÅîÌïòÍ≤å ÏãúÍ∞ÅÌôî ‚îÄ‚îÄ‚îÄ\u001b[39;00m\n\u001b[0;32m     57\u001b[0m az\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marviz-darkgrid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m \u001b[43maz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_forest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43midata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvar_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mŒ≤_age\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mŒ≤_meth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mŒ≤_prev\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mœÉ_sr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mœÉ_r\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mœÉ_c\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mœÉ_prev_study\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mœÉ_y\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombined\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcredible_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mridgeplot_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\n\u001b[0;32m     64\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: plot_forest() got an unexpected keyword argument 'credible_interval'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with pm.Model(coords=coords) as model:\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ Fixed effects ‚îÄ‚îÄ‚îÄ\n",
    "    Œ≤0    = pm.Normal(\"Œ≤0\",    mu=0, sigma=10)\n",
    "    Œ≤_age = pm.Normal(\"Œ≤_age\", mu=0, sigma=5, dims=\"age\")\n",
    "    Œ≤_meth= pm.Normal(\"Œ≤_meth\",mu=0, sigma=5, dims=\"meth\")\n",
    "    Œ≤_prev= pm.Normal(\"Œ≤_prev\",mu=0, sigma=5, dims=\"prev\")\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ Non-centered random intercepts ‚îÄ‚îÄ‚îÄ\n",
    "    œÉ_sr = pm.HalfNormal(\"œÉ_sr\", sigma=5)\n",
    "    Œ∑_sr = pm.Normal(\"Œ∑_sr\", mu=0, sigma=1, dims=\"super_region\")\n",
    "    u_sr = pm.Deterministic(\"u_sr\", Œ∑_sr * œÉ_sr)\n",
    "\n",
    "    œÉ_r  = pm.HalfNormal(\"œÉ_r\", sigma=5)\n",
    "    Œ∑_r  = pm.Normal(\"Œ∑_r\",  mu=0, sigma=1, dims=\"region\")\n",
    "    u_r  = pm.Deterministic(\"u_r\", Œ∑_r * œÉ_r)\n",
    "\n",
    "    œÉ_c  = pm.HalfNormal(\"œÉ_c\", sigma=5)\n",
    "    Œ∑_c  = pm.Normal(\"Œ∑_c\",  mu=0, sigma=1, dims=\"country\")\n",
    "    u_c  = pm.Deterministic(\"u_c\", Œ∑_c * œÉ_c)\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ Study-level random slope (non-centered) ‚îÄ‚îÄ‚îÄ\n",
    "    œÉ_ps= pm.HalfNormal(\"œÉ_prev_study\", sigma=5)\n",
    "    Œ∑_ps= pm.Normal(\"Œ∑_prev_study\", mu=0, sigma=1, dims=(\"study\",\"prev\"))\n",
    "    u_ps= pm.Deterministic(\"u_prev_study\", Œ∑_ps * œÉ_ps)\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ ÏÑ†Ìòï ÏòàÏ∏°Ïãù Œº ‚îÄ‚îÄ‚îÄ\n",
    "    prev_effect = (Œ≤_prev[prev_idx] + u_ps[study_idx,prev_idx]) * multi_mask\n",
    "\n",
    "    Œº = (\n",
    "        Œ≤0\n",
    "        + Œ≤_age[age_idx]\n",
    "        + Œ≤_meth[meth_idx]\n",
    "        + prev_effect\n",
    "        + u_sr[super_idx]\n",
    "        + u_r[region_idx]\n",
    "        + u_c[country_idx]\n",
    "    )\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ Í¥ÄÏ∏°Î™®Ìòï ‚îÄ‚îÄ‚îÄ\n",
    "    œÉ_y = pm.HalfNormal(\"œÉ_y\", sigma=1)\n",
    "    pm.Normal(\"y\", mu=Œº, sigma=œÉ_y, observed=y_obs)\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ (ÏòµÏÖò) ADVI ÏõåÎ∞çÏóÖ ‚Äî Îπ†Î•¥Í≤å Í∑ºÏÇ¨ posterior ÏÉùÏÑ± ‚îÄ‚îÄ‚îÄ\n",
    "    advi = pm.fit(method=\"advi\", n=5000)\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ NUTS ÏÉòÌîåÎßÅ ‚îÄ‚îÄ‚îÄ\n",
    "    idata = pm.sample(\n",
    "        draws=1000, tune=500,\n",
    "        chains=4, cores=4,\n",
    "        target_accept=0.95,\n",
    "        max_treedepth=15,\n",
    "        progressbar=\"split\"\n",
    "    )\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ forest plot ÏúºÎ°ú ÍπîÎÅîÌïòÍ≤å ÏãúÍ∞ÅÌôî ‚îÄ‚îÄ‚îÄ\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "az.plot_forest(\n",
    "    idata,\n",
    "    var_names=[\"Œ≤_age\",\"Œ≤_meth\",\"Œ≤_prev\",\"œÉ_sr\",\"œÉ_r\",\"œÉ_c\",\"œÉ_prev_study\",\"œÉ_y\"],\n",
    "    combined=True,\n",
    "    credible_interval=0.95,\n",
    "    ridgeplot_overlap=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76eca402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               mean     sd  hdi_2.5%  hdi_97.5%  mcse_mean  \\\n",
      "Œ≤0                           -3.537  4.085   -11.136      4.469      0.111   \n",
      "Œ≤_age[Children]              -0.263  2.759    -5.733      4.756      0.080   \n",
      "Œ≤_age[Adult]                 -0.284  2.752    -5.538      4.946      0.080   \n",
      "Œ≤_age[Both]                  -0.183  2.763    -5.575      4.984      0.080   \n",
      "Œ≤_meth[Interview based]       0.241  3.283    -6.431      6.356      0.080   \n",
      "Œ≤_meth[Non-interview]        -0.924  3.280    -7.457      5.359      0.080   \n",
      "Œ≤_prev[point prevalence]     -1.014  0.388    -1.760     -0.244      0.006   \n",
      "Œ≤_prev[12-month prevalence]  -0.401  0.291    -0.957      0.177      0.006   \n",
      "Œ≤_prev[life-time prevalence] -0.121  0.290    -0.680      0.434      0.006   \n",
      "œÉ_sr                          0.916  0.575     0.008      1.955      0.017   \n",
      "œÉ_r                           0.291  0.242     0.000      0.755      0.008   \n",
      "œÉ_c                           0.455  0.218     0.011      0.816      0.010   \n",
      "œÉ_prev_study                  0.182  0.139     0.000      0.449      0.003   \n",
      "œÉ_y                           0.881  0.080     0.731      1.039      0.002   \n",
      "\n",
      "                              mcse_sd  ess_bulk  ess_tail  r_hat  \n",
      "Œ≤0                              0.059  1349.819  2156.025  1.001  \n",
      "Œ≤_age[Children]                 0.042  1195.568  1998.628  1.002  \n",
      "Œ≤_age[Adult]                    0.042  1198.977  1947.445  1.002  \n",
      "Œ≤_age[Both]                     0.042  1187.404  1955.283  1.003  \n",
      "Œ≤_meth[Interview based]         0.047  1672.714  2359.159  1.001  \n",
      "Œ≤_meth[Non-interview]           0.047  1664.701  2388.832  1.002  \n",
      "Œ≤_prev[point prevalence]        0.006  3568.001  2825.607  1.001  \n",
      "Œ≤_prev[12-month prevalence]     0.004  2734.519  2586.475  1.001  \n",
      "Œ≤_prev[life-time prevalence]    0.004  2495.347  2760.945  1.001  \n",
      "œÉ_sr                            0.016   851.519   857.021  1.007  \n",
      "œÉ_r                             0.007  1035.500  1252.875  1.007  \n",
      "œÉ_c                             0.004   501.880   695.262  1.008  \n",
      "œÉ_prev_study                    0.002  1607.233  1519.844  1.001  \n",
      "œÉ_y                             0.001  1135.781  2415.758  1.002  \n"
     ]
    }
   ],
   "source": [
    "import arviz as az\n",
    "\n",
    "# idataÎäî Ïù¥ÎØ∏ pm.sample()Ïùò Í≤∞Í≥º\n",
    "summary_df = az.summary(\n",
    "    idata,\n",
    "    var_names=[\n",
    "        \"Œ≤0\", \"Œ≤_age\", \"Œ≤_meth\", \"Œ≤_prev\",\n",
    "        \"œÉ_sr\", \"œÉ_r\", \"œÉ_c\", \"œÉ_prev_study\", \"œÉ_y\"\n",
    "    ],\n",
    "    round_to=3,         # ÏÜåÏàòÏ†ê ÏûêÎ¶¨Ïàò\n",
    "    hdi_prob=0.95       # 95% Ïã†Î¢∞Íµ¨Í∞Ñ\n",
    ")\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a21a0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to C:/Users/Ï†ïÏù¥Îì†/Desktop/Ïó∞Íµ¨Ïã§/9. prevalence of OCD/3Ï∞® ÏôÄÍæ∏/life_time_prevalence.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 1) posteriorÎ•º (chain, draw) ‚Üí sample Ï∞®Ïõê ÌïòÎÇòÎ°ú Ìï©ÏπòÍ∏∞\n",
    "post = idata.posterior.stack(sample=(\"chain\",\"draw\"))\n",
    "\n",
    "# 2) coordsÏóê Ï†ÄÏû•Îêú Î¶¨Ïä§Ìä∏ Î∂àÎü¨Ïò§Í∏∞\n",
    "country_list      = coords[\"country\"]       # Level-3 Íµ≠Í∞Ä Ïù¥Î¶Ñ Î¶¨Ïä§Ìä∏\n",
    "super_list        = coords[\"super_region\"]  # Level-1 ÏäàÌçºÎ¶¨Ï†Ñ\n",
    "region_list       = coords[\"region\"]        # Level-2 Î¶¨Ï†Ñ\n",
    "age_list          = coords[\"age\"]           # [\"Children\",\"Adult\",\"Both\"]\n",
    "meth_list         = coords[\"meth\"]          # [\"Interview based\",\"Non-interview\"]\n",
    "prev_list         = coords[\"prev\"]          # [\"point prevalence\",\"12-month prevalence\",\"life-time prevalence\"]\n",
    "\n",
    "# 3) Í¥ÄÏã¨ Î≤îÏ£ºÏùò posterior ÏÉòÌîå Í∫ºÎÇ¥Í∏∞\n",
    "Œ≤0_samps         = post[\"Œ≤0\"].values                             # (samples,)\n",
    "Œ≤_age_Both       = post[\"Œ≤_age\"].sel(age=\"Both\").values          # (samples,)\n",
    "Œ≤_meth_IB        = post[\"Œ≤_meth\"].sel(meth=\"Interview based\").values  # (samples,)\n",
    "Œ≤_prev_life      = post[\"Œ≤_prev\"].sel(prev=\"life-time prevalence\").values  # (samples,)\n",
    "\n",
    "u_sr_samps       = post[\"u_sr\"].values      # (super_region, samples)\n",
    "u_r_samps        = post[\"u_r\"].values       # (region, samples)\n",
    "u_c_samps        = post[\"u_c\"].values       # (country, samples)\n",
    "\n",
    "# 4) country ‚Üí region, super_region Îß§Ìïë (levels ÌååÏùºÏóêÏÑú ÎØ∏Î¶¨ ÎßåÎì§Ïñ¥ ÎëêÏóàÎã§Î©¥ Í∑∏Í≤É ÏÇ¨Ïö©)\n",
    "#    Ïó¨Í∏∞ÏÑ† hier Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏóê ÏûàÎäî Level, Location Name, Parent ID Ïù¥Ïö©\n",
    "mapping = hier[[\"Level\",\"Location ID\",\"Location Name\",\"Parent ID\"]]\n",
    "# Level3\n",
    "lvl3 = mapping[mapping[\"Level\"]==3].rename(columns={\n",
    "    \"Location Name\":\"country\",\"Location ID\":\"loc3\",\"Parent ID\":\"parent2\"\n",
    "})\n",
    "# Level2\n",
    "lvl2 = mapping[mapping[\"Level\"]==2].rename(columns={\n",
    "    \"Location Name\":\"region\",\"Location ID\":\"loc2\",\"Parent ID\":\"parent1\"\n",
    "})\n",
    "# Level1\n",
    "lvl1 = mapping[mapping[\"Level\"]==1].rename(columns={\n",
    "    \"Location Name\":\"super_region\",\"Location ID\":\"loc1\"\n",
    "})\n",
    "\n",
    "# merge to get each country‚Äôs region & super_region names\n",
    "df_map = (\n",
    "    lvl3[[\"country\",\"loc3\",\"parent2\"]]\n",
    "    .merge(lvl2[[\"loc2\",\"region\",\"parent1\"]], left_on=\"parent2\", right_on=\"loc2\", how=\"left\")\n",
    "    .merge(lvl1[[\"loc1\",\"super_region\"]], left_on=\"parent1\", right_on=\"loc1\", how=\"left\")\n",
    "    [[\"country\",\"region\",\"super_region\"]]\n",
    ")\n",
    "\n",
    "# 5) Monte Carlo ÏÉòÌîåÎßàÎã§ Œº Í≥ÑÏÇ∞ ‚Üí p = exp(Œº)\n",
    "results = []\n",
    "n_samps = Œ≤0_samps.shape[0]\n",
    "for idx, row in df_map.iterrows():\n",
    "    country    = row[\"country\"]\n",
    "    region     = row[\"region\"]\n",
    "    super_reg  = row[\"super_region\"]\n",
    "    i_sr = super_list.index(super_reg)\n",
    "    i_r  = region_list.index(region)\n",
    "    i_c  = country_list.index(country)\n",
    "\n",
    "    # Œº ÏÉòÌîå Î≤°ÌÑ∞\n",
    "    mu = (\n",
    "        Œ≤0_samps\n",
    "        + Œ≤_age_Both\n",
    "        + Œ≤_meth_IB\n",
    "        + Œ≤_prev_life\n",
    "        + u_sr_samps[i_sr, :]\n",
    "        + u_r_samps[i_r, :]\n",
    "        + u_c_samps[i_c, :]\n",
    "    )\n",
    "    # prevalence ÎπÑÏú®Î°ú Î≥ÄÌôò\n",
    "    p = np.exp(mu)\n",
    "\n",
    "    # median & 95% HDI\n",
    "    med = np.median(p)\n",
    "    hdi_lo, hdi_hi = az.hdi(p, hdi_prob=0.95)\n",
    "\n",
    "    results.append({\n",
    "        \"country\":             country,\n",
    "        \"region\":              region,\n",
    "        \"super_region\":        super_reg,\n",
    "        \"median_life_prev\":    med,\n",
    "        \"hdi_2.5%\":            hdi_lo,\n",
    "        \"hdi_97.5%\":           hdi_hi\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(results)\n",
    "\n",
    "# 6) ÏóëÏÖÄÎ°ú Ï†ÄÏû•\n",
    "output_path = \"C:/Users/Ï†ïÏù¥Îì†/Desktop/Ïó∞Íµ¨Ïã§/9. prevalence of OCD/3Ï∞® ÏôÄÍæ∏/life_time_prevalence.xlsx\"\n",
    "df_out.to_excel(output_path, index=False)\n",
    "print(f\"Saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
