{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a10dd0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b42542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonc(filepath):\n",
    "    \"\"\"Load JSONC file (JSON with comments)\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Remove single-line comments (// ...)\n",
    "    content = re.sub(r'//.*?$', '', content, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove multi-line comments (/* ... */)\n",
    "    content = re.sub(r'/\\*.*?\\*/', '', content, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove trailing commas before closing brackets/braces\n",
    "    content = re.sub(r',\\s*([}\\]])', r'\\1', content)\n",
    "    \n",
    "    return json.loads(content)\n",
    "\n",
    "def load_json(filepath):\n",
    "    \"\"\"Load regular JSON file\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_any(filepath):\n",
    "    \"\"\"Load either JSON or JSONC file based on extension\"\"\"\n",
    "    if filepath.endswith('.jsonc'):\n",
    "        return load_jsonc(filepath)\n",
    "    else:\n",
    "        return load_json(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdfc5e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amd_sim_data                   parameters keys: dict_keys(['rr', 'f', 'i', 'age_weights', 'ages', 'p', 'r', 'pf', 'X'])\n",
      "amd_sim_data_Early             parameters keys: dict_keys(['rr', 'f', 'i', 'age_weights', 'ages', 'p', 'r', 'pf', 'X'])\n",
      "amd_sim_data_Intermediate      parameters keys: dict_keys(['rr', 'f', 'i', 'age_weights', 'ages', 'p', 'r', 'pf', 'X'])\n",
      "amd_sim_data_Late-dry          parameters keys: dict_keys(['rr', 'f', 'i', 'age_weights', 'ages', 'p', 'r', 'pf', 'X'])\n",
      "amd_sim_data_Late-wet          parameters keys: dict_keys(['rr', 'f', 'i', 'age_weights', 'ages', 'p', 'r', 'pf', 'X'])\n"
     ]
    }
   ],
   "source": [
    "# TASK 1: relocate age_weights to parameters.jsonc file\n",
    "input_dir = './input_data'\n",
    "stage_dirs = ['amd_sim_data', 'amd_sim_data_Early', 'amd_sim_data_Intermediate', 'amd_sim_data_Late-dry', 'amd_sim_data_Late-wet']\n",
    "\n",
    "for stage_dir in stage_dirs:\n",
    "    parameters = load_jsonc(f'{input_dir}/{stage_dir}/parameters.jsonc')\n",
    "    print(f'{stage_dir:<30} parameters keys: {parameters.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97c4128b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows: 207\n",
      "number of unique location_id: 18\n",
      "--------------------------------\n",
      "number of nodes: 233\n",
      "number of edges: 232\n"
     ]
    }
   ],
   "source": [
    "# TASK 2: update Load based on region_id_graph\n",
    "stage_dirs = ['amd_sim_data', 'amd_sim_data_Early', 'amd_sim_data_Intermediate', 'amd_sim_data_Late-dry', 'amd_sim_data_Late-wet']\n",
    "stage = stage_dirs[2] # Intermediate\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------#\n",
    "# Read input data\n",
    "\n",
    "input_data = pd.read_csv(f'{input_dir}/{stage}/input_data.csv')\n",
    "output_template = pd.read_csv(f'{input_dir}/{stage}/output_template.csv')\n",
    "parameters = load_any(f'{input_dir}/{stage}/parameters.jsonc')    # dict. {'p': {}, 'age_weights': [], 'ages: []}\n",
    "hierarchy = load_any(f'{input_dir}/{stage}/hierarchy.json')       # dict of lists. {'nodes': [], 'edges': []}\n",
    "nodes_to_fit = load_any(f'{input_dir}/{stage}/nodes_to_fit.json') # LIST of strings\n",
    "\n",
    "print(f'number of rows: {len(input_data)}')\n",
    "print(f'number of unique location_id: {input_data[\"location_id\"].nunique()}')\n",
    "print('--------------------------------')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------#\n",
    "# Create region_graph\n",
    "nodes = hierarchy['nodes']\n",
    "name_to_id = {} # warning: this does not handle duplicate names\n",
    "id_to_name = {}\n",
    "\n",
    "region_id_graph = nx.DiGraph()\n",
    "\n",
    "for node in nodes:\n",
    "  name_to_id[node[0]] = node[1]['location_id']\n",
    "  id_to_name[node[1]['location_id']] = node[0]\n",
    "\n",
    "  # create region_id_graph\n",
    "  region_id_graph.add_node(node[1]['location_id'],\n",
    "                            level = node[1]['level'],\n",
    "                            parent_id = node[1]['parent_id'],\n",
    "                            name = node[0]\n",
    "                          )\n",
    "\n",
    "  my_id = node[1]['location_id']\n",
    "  parent_id = node[1]['parent_id']\n",
    "  if my_id != parent_id: # if my_id is not the root node\n",
    "    region_id_graph.add_edge(parent_id, my_id)\n",
    "\n",
    "print(f\"number of nodes: {region_id_graph.number_of_nodes()}\") \n",
    "print(f\"number of edges: {region_id_graph.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cba1b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States Virgin Islands\n",
      "3\n",
      "104\n",
      "United States Virgin Islands\n"
     ]
    }
   ],
   "source": [
    "print(id_to_name[422]) # IDs are not incremental\n",
    "\n",
    "print(region_id_graph.nodes[422]['level'])\n",
    "print(region_id_graph.nodes[422]['parent_id'])\n",
    "print(region_id_graph.nodes[422]['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "826289aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "21\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# TASK 3: define \"coords\"\n",
    "\n",
    "country_list = []\n",
    "region_list = []\n",
    "super_region_list = []\n",
    "\n",
    "for node in hierarchy['nodes']:\n",
    "  if node[1]['level'] == 3:\n",
    "    country_list.append(node[1]['location_id'])\n",
    "  elif node[1]['level'] == 2:\n",
    "    region_list.append(node[1]['location_id'])\n",
    "  elif node[1]['level'] == 1:\n",
    "    super_region_list.append(node[1]['location_id'])\n",
    "    \n",
    "coords = {\n",
    "    \"country\":      country_list,\n",
    "    \"region\":       region_list,\n",
    "    \"super_region\": super_region_list,\n",
    "}\n",
    "\n",
    "print(len(country_list))\n",
    "print(len(region_list))\n",
    "print(len(super_region_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "590211ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>location_id</th>\n",
       "      <th>stage</th>\n",
       "      <th>stage_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>sex_id</th>\n",
       "      <th>year_id</th>\n",
       "      <th>age_start</th>\n",
       "      <th>age_end</th>\n",
       "      <th>effective_sample_size</th>\n",
       "      <th>value</th>\n",
       "      <th>standard_error</th>\n",
       "      <th>x_sdi</th>\n",
       "      <th>x_tob</th>\n",
       "      <th>data_type</th>\n",
       "      <th>upper_ci</th>\n",
       "      <th>lower_ci</th>\n",
       "      <th>age_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>89</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>5</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>1990</td>\n",
       "      <td>55</td>\n",
       "      <td>64</td>\n",
       "      <td>1418.0</td>\n",
       "      <td>0.040903</td>\n",
       "      <td>0.005260</td>\n",
       "      <td>0.794612</td>\n",
       "      <td>0.434156</td>\n",
       "      <td>p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>89</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>5</td>\n",
       "      <td>Female</td>\n",
       "      <td>2</td>\n",
       "      <td>1990</td>\n",
       "      <td>55</td>\n",
       "      <td>64</td>\n",
       "      <td>1802.0</td>\n",
       "      <td>0.033851</td>\n",
       "      <td>0.004260</td>\n",
       "      <td>0.794612</td>\n",
       "      <td>0.383810</td>\n",
       "      <td>p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>89</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>5</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>1990</td>\n",
       "      <td>65</td>\n",
       "      <td>74</td>\n",
       "      <td>1382.0</td>\n",
       "      <td>0.072359</td>\n",
       "      <td>0.006969</td>\n",
       "      <td>0.794612</td>\n",
       "      <td>0.434156</td>\n",
       "      <td>p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>89</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>5</td>\n",
       "      <td>Female</td>\n",
       "      <td>2</td>\n",
       "      <td>1990</td>\n",
       "      <td>65</td>\n",
       "      <td>74</td>\n",
       "      <td>1865.0</td>\n",
       "      <td>0.036997</td>\n",
       "      <td>0.004371</td>\n",
       "      <td>0.794612</td>\n",
       "      <td>0.383810</td>\n",
       "      <td>p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>89</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>5</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>1990</td>\n",
       "      <td>75</td>\n",
       "      <td>84</td>\n",
       "      <td>796.0</td>\n",
       "      <td>0.103015</td>\n",
       "      <td>0.010774</td>\n",
       "      <td>0.794612</td>\n",
       "      <td>0.434156</td>\n",
       "      <td>p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          area  location_id         stage  stage_id     sex  sex_id  year_id   \n",
       "0  Netherlands           89  Intermediate         5    Male       1     1990  \\\n",
       "1  Netherlands           89  Intermediate         5  Female       2     1990   \n",
       "2  Netherlands           89  Intermediate         5    Male       1     1990   \n",
       "3  Netherlands           89  Intermediate         5  Female       2     1990   \n",
       "4  Netherlands           89  Intermediate         5    Male       1     1990   \n",
       "\n",
       "   age_start  age_end  effective_sample_size     value  standard_error   \n",
       "0         55       64                 1418.0  0.040903        0.005260  \\\n",
       "1         55       64                 1802.0  0.033851        0.004260   \n",
       "2         65       74                 1382.0  0.072359        0.006969   \n",
       "3         65       74                 1865.0  0.036997        0.004371   \n",
       "4         75       84                  796.0  0.103015        0.010774   \n",
       "\n",
       "      x_sdi     x_tob data_type  upper_ci  lower_ci  age_weights  \n",
       "0  0.794612  0.434156         p       NaN       NaN          NaN  \n",
       "1  0.794612  0.383810         p       NaN       NaN          NaN  \n",
       "2  0.794612  0.434156         p       NaN       NaN          NaN  \n",
       "3  0.794612  0.383810         p       NaN       NaN          NaN  \n",
       "4  0.794612  0.434156         p       NaN       NaN          NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utilizes id_to_name to print the name of the location\n",
    "def describe():\n",
    "        G = region_id_graph\n",
    "        df = input_data\n",
    "        for n in nx.dfs_postorder_nodes(G, 1):\n",
    "            cnt = df['location_id'].eq(n).sum() + sum(G.nodes[c].get('cnt', 0) for c in G.successors(n))\n",
    "            G.nodes[n]['cnt'] = int(cnt)\n",
    "            G.nodes[n]['depth'] = nx.shortest_path_length(G, 1, n)\n",
    "            \n",
    "        for n in nx.dfs_preorder_nodes(G, 1):\n",
    "            if G.nodes[n]['cnt'] > 0:\n",
    "                print('  '*G.nodes[n]['depth'] + id_to_name[n], G.nodes[n]['cnt'])\n",
    "\n",
    "# describe()\n",
    "\n",
    "def keep():\n",
    "    pass\n",
    "    # Suggestion: filter input_data during \"LOAD\"\n",
    "\n",
    "def filter_input_data_by_data_type(input_data: pd.DataFrame, data_type: str) -> pd.DataFrame:\n",
    "        if not input_data.empty:\n",
    "            return input_data[input_data['data_type'] == data_type]\n",
    "        return input_data\n",
    "\n",
    "# since our input_data only has 'p' data, this will return the same input_data\n",
    "filter_input_data_by_data_type(input_data, 'p').head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b5d9f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process.asr() | parameters\n",
    "# ------------------------------------------------------------\n",
    "data_type                = 'p'\n",
    "reference_area           = 'Global'\n",
    "reference_sex            = 'Both'\n",
    "reference_year           = 'all'\n",
    "mu_age                   = None\n",
    "mu_age_parent            = None\n",
    "sigma_age_parent         = None\n",
    "rate_type                = 'neg_binom'\n",
    "lower_bound              = None\n",
    "interpolation_method     = 'linear'\n",
    "include_covariates       = True\n",
    "zero_re                  = False\n",
    "# ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dc3ce0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ages: [ 2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19.\n",
      " 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37.\n",
      " 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54. 55.\n",
      " 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71. 72. 73.\n",
      " 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89. 90. 91.\n",
      " 92. 93. 94.] | type: <class 'numpy.ndarray'> | ages.dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# process.asr() | local variables\n",
    "# ------------------------------------------------------------\n",
    "ages = np.array(parameters['ages'], dtype=np.float64)\n",
    "age_weights = np.array(parameters['age_weights'], dtype=np.float64)\n",
    "data = filter_input_data_by_data_type(input_data, data_type)\n",
    "lb_data =filter_input_data_by_data_type(input_data, lower_bound) if lower_bound else None\n",
    "params_of_data_type = parameters.get(data_type, {})\n",
    "\n",
    "# check: mu_age_parent & sigma_age_parent\n",
    "#  if either mu_age_parent or sigma_age_parent is NaN, set them to None\n",
    "if (isinstance(mu_age_parent, np.ndarray) and np.isnan(mu_age_parent).any()) or \\\n",
    "    (isinstance(sigma_age_parent, np.ndarray) and np.isnan(sigma_age_parent).any()):\n",
    "\n",
    "    mu_age_parent = None\n",
    "    sigma_age_parent = None\n",
    "# ------------------------------------------------------------\n",
    "print(f'ages: {ages} | type: {type(ages)} | ages.dtype: {ages.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "449ad149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knots: [ 2. 30. 45. 60. 80. 94.] | type: <class 'numpy.ndarray'> | knots.dtype: float64\n",
      "smoothing: 0.5\n"
     ]
    }
   ],
   "source": [
    "# process.asr() | [1] Prepare spline.spline (knots, smoothing)\n",
    "# ------------------------------------------------------------\n",
    "knots = np.array(params_of_data_type.get('parameter_age_mesh', np.arange(ages[0], ages[-1] + 1, 5)), dtype=np.float64)\n",
    "\n",
    "smooth_map = {'No Prior': np.inf, 'Slightly': 0.5, 'Moderately': 0.05, 'Very': 0.005}\n",
    "smoothness_param = params_of_data_type.get('smoothness')\n",
    "\n",
    "if isinstance(smoothness_param, dict): \n",
    "    amount = smoothness_param.get('amount')\n",
    "\n",
    "    if isinstance(amount, (int, float)): # smoothness_param is dict, and amount is int or float\n",
    "        smoothing = float(amount)\n",
    "    else:                                # smoothness_param is dict, and amount may be string\n",
    "        smoothing = smooth_map.get(amount, 0.0)\n",
    "\n",
    "else:                                    # smoothness_param may be string\n",
    "    smoothing = smooth_map.get(smoothness_param, 0.0)\n",
    "# ------------------------------------------------------------\n",
    "print(f'knots: {knots} | type: {type(knots)} | knots.dtype: {knots.dtype}')\n",
    "print(f'smoothing: {smoothing}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b23f5f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_model(model, var_name=None, show_shared_data=True):\n",
    "    \"\"\"\n",
    "    Inspect a PyMC model. If var_name is None, print a summary,\n",
    "    plus any shared_data contents. Otherwise, show details about a specific variable.\n",
    "    \"\"\"\n",
    "    if var_name is None:\n",
    "        print(\"📊 Model Summary:\")\n",
    "        print(f\"  • Free RVs       : {len(model.free_RVs)} {[rv.name for rv in model.free_RVs]}\")\n",
    "        print(f\"  • Observed RVs   : {len(model.observed_RVs)} {[rv.name for rv in model.observed_RVs]}\")\n",
    "        print(f\"  • Deterministics : {len(model.deterministics)} {[rv.name for rv in model.deterministics]}\")\n",
    "        print(f\"  • Potentials     : {len(model.potentials)} {[pot.name for pot in model.potentials]}\")\n",
    "        print(f\"  • Total Named RVs: {len(model.named_vars)}\")\n",
    "\n",
    "        # --- Print shared_data contents if present ---\n",
    "        if show_shared_data:\n",
    "            if hasattr(model, \"shared_data\"):\n",
    "                sd = model.shared_data\n",
    "                if isinstance(sd, dict) and sd:\n",
    "                    print(\"\\n🔖 shared_data:\")\n",
    "                    for key, val in sd.items():\n",
    "                        if isinstance(val, np.ndarray):\n",
    "                            print(f\"  • {key:15s}: array, shape={val.shape}, dtype={val.dtype}\")\n",
    "                        else:\n",
    "                            print(f\"  • {key:15s}: {val!r}\")\n",
    "\n",
    "    else:\n",
    "        var_dict = model.named_vars\n",
    "        if var_name not in var_dict:\n",
    "            print(f\"❌ Variable '{var_name}' not found in model.named_vars.\")\n",
    "            return\n",
    "\n",
    "        var = var_dict[var_name]\n",
    "        print(f\"🔍 Variable: {var_name}\")\n",
    "        print(f\"  • Type     : {type(var)}\")\n",
    "        print(f\"  • Shape    : {getattr(var, 'shape', None)}\")\n",
    "        print(f\"  • DType    : {getattr(var, 'dtype', None)}\")\n",
    "        print(f\"  • Owner OP : {var.owner.op if getattr(var, 'owner', None) else 'None'}\")\n",
    "\n",
    "        if hasattr(var, 'distribution'):\n",
    "            dist = var.distribution\n",
    "            print(f\"  • Distribution: {dist.__class__.__name__}\")\n",
    "            if hasattr(dist, 'dist'):\n",
    "                print(f\"    - PyMC Dist : {dist.dist.__class__.__name__}\")\n",
    "            if hasattr(dist, 'kwargs'):\n",
    "                print(\"    - Parameters:\")\n",
    "                for k, v in dist.kwargs.items():\n",
    "                    print(f\"      {k}: {v}\")\n",
    "\n",
    "        if hasattr(var, 'eval'):\n",
    "            try:\n",
    "                val = var.eval()\n",
    "                print(f\"  • Current value (eval): {val}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  • Could not evaluate variable: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bd8a4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Model Summary:\n",
      "  • Free RVs       : 0 []\n",
      "  • Observed RVs   : 0 []\n",
      "  • Deterministics : 0 []\n",
      "  • Potentials     : 0 []\n",
      "  • Total Named RVs: 0\n",
      "\n",
      "🔖 shared_data:\n",
      "  • data_type      : 'p'\n",
      "  • ages           : array, shape=(93,), dtype=float64\n",
      "  • age_weights    : array, shape=(101,), dtype=float64\n",
      "  • data           :                          area  location_id         stage  stage_id     sex   \n",
      "0                 Netherlands           89  Intermediate         5    Male  \\\n",
      "1                 Netherlands           89  Intermediate         5  Female   \n",
      "2                 Netherlands           89  Intermediate         5    Male   \n",
      "3                 Netherlands           89  Intermediate         5  Female   \n",
      "4                 Netherlands           89  Intermediate         5    Male   \n",
      "..                        ...          ...           ...       ...     ...   \n",
      "202                   Ireland           84  Intermediate         5    Both   \n",
      "203                   Germany           81  Intermediate         5    Both   \n",
      "204        Russian Federation           62  Intermediate         5    Both   \n",
      "205  United States of America          102  Intermediate         5    Both   \n",
      "206  United States of America          102  Intermediate         5    Both   \n",
      "\n",
      "     sex_id  year_id  age_start  age_end  effective_sample_size     value   \n",
      "0         1     1990         55       64                 1418.0  0.040903  \\\n",
      "1         2     1990         55       64                 1802.0  0.033851   \n",
      "2         1     1990         65       74                 1382.0  0.072359   \n",
      "3         2     1990         65       74                 1865.0  0.036997   \n",
      "4         1     1990         75       84                  796.0  0.103015   \n",
      "..      ...      ...        ...      ...                    ...       ...   \n",
      "202       3     2013         85       99                   47.0  0.170213   \n",
      "203       3     2016         50       95                 4016.0  0.086155   \n",
      "204       3     2017         85       98                  932.0  0.198498   \n",
      "205       3     2018         65       95              2175803.0  0.016000   \n",
      "206       3     2019         65       95              2445163.0  0.016000   \n",
      "\n",
      "     standard_error     x_sdi     x_tob data_type  upper_ci  lower_ci   \n",
      "0          0.005260  0.794612  0.434156         p       NaN       NaN  \\\n",
      "1          0.004260  0.794612  0.383810         p       NaN       NaN   \n",
      "2          0.006969  0.794612  0.434156         p       NaN       NaN   \n",
      "3          0.004371  0.794612  0.383810         p       NaN       NaN   \n",
      "4          0.010774  0.794612  0.434156         p       NaN       NaN   \n",
      "..              ...       ...       ...       ...       ...       ...   \n",
      "202        0.054819  0.838644  0.251643         p       NaN       NaN   \n",
      "203        0.004428  0.890896  0.268396         p       NaN       NaN   \n",
      "204        0.013065  0.795669  0.301889         p       NaN       NaN   \n",
      "205        0.000085  0.855049  0.177130         p       NaN       NaN   \n",
      "206        0.000080  0.858578  0.175695         p       NaN       NaN   \n",
      "\n",
      "     age_weights  \n",
      "0            NaN  \n",
      "1            NaN  \n",
      "2            NaN  \n",
      "3            NaN  \n",
      "4            NaN  \n",
      "..           ...  \n",
      "202          NaN  \n",
      "203          NaN  \n",
      "204          NaN  \n",
      "205          NaN  \n",
      "206          NaN  \n",
      "\n",
      "[207 rows x 18 columns]\n",
      "  • lb_data        : None\n",
      "  • knots          : array, shape=(6,), dtype=float64\n",
      "  • smoothing      : 0.5\n",
      "  • interpolation_method: 'linear'\n",
      "  • params_of_data_type: {'increasing': {'age_start': 2, 'age_end': 94}, 'decreasing': {'age_start': 0, 'age_end': 0}, 'fixed_effects': {}, 'random_effects': {}, 'level_bounds': {'lower': 0.0, 'upper': 1.0}, 'y_maximum': 1.0, 'level_value': {'age_before': 30, 'age_after': 100, 'value': 0.0}, 'parameter_age_mesh': [2, 30, 45, 60, 80, 94], 'heterogeneity': 'Very', 'smoothness': {'age_start': 2, 'amount': 'Slightly', 'age_end': 94}}\n",
      "  • reference_area_id: 1\n",
      "  • reference_sex  : 'Both'\n",
      "  • reference_year : 'all'\n",
      "  • zero_re        : False\n",
      "  • region_id_graph: <networkx.classes.digraph.DiGraph object at 0x17915e2e0>\n",
      "  • output_template:        location_id   area  year  sex_id     sex           pop     x_tob   \n",
      "0                6  China  1990       1    Male  4.028249e+09  0.608153  \\\n",
      "1                6  China  1991       1    Male  4.077079e+09  0.606039   \n",
      "2                6  China  1992       1    Male  4.124761e+09  0.602314   \n",
      "3                6  China  1993       1    Male  4.170702e+09  0.596882   \n",
      "4                6  China  1994       1    Male  4.215222e+09  0.589594   \n",
      "...            ...    ...   ...     ...     ...           ...       ...   \n",
      "12235          522  Sudan  2015       2  Female  1.220968e+08  0.020120   \n",
      "12236          522  Sudan  2016       2  Female  1.253744e+08  0.019630   \n",
      "12237          522  Sudan  2017       2  Female  1.283511e+08  0.019547   \n",
      "12238          522  Sudan  2018       2  Female  1.308981e+08  0.019498   \n",
      "12239          522  Sudan  2019       2  Female  1.333941e+08  0.019456   \n",
      "\n",
      "          x_sdi  \n",
      "0      0.458669  \n",
      "1      0.467302  \n",
      "2      0.475784  \n",
      "3      0.484354  \n",
      "4      0.492579  \n",
      "...         ...  \n",
      "12235  0.486549  \n",
      "12236  0.496848  \n",
      "12237  0.506884  \n",
      "12238  0.516214  \n",
      "12239  0.525009  \n",
      "\n",
      "[12240 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "pm_model = pm.Model()\n",
    "\n",
    "with pm_model: \n",
    "    pm_model.shared_data = {    # NOTE: this is what used to be \"vars\" from class ModelVars\n",
    "        \"data_type\": data_type,\n",
    "        \"ages\":      ages,\n",
    "        \"age_weights\": age_weights,\n",
    "        \"data\":      data,\n",
    "        \"lb_data\":   lb_data,\n",
    "        \"knots\":     knots,\n",
    "        \"smoothing\": smoothing,\n",
    "        \"interpolation_method\": interpolation_method,\n",
    "        \"params_of_data_type\": params_of_data_type,\n",
    "        \"reference_area_id\": name_to_id[reference_area],\n",
    "        \"reference_sex\": reference_sex,\n",
    "        \"reference_year\": reference_year,\n",
    "        \"zero_re\": zero_re,\n",
    "        \"region_id_graph\": region_id_graph,\n",
    "        \"output_template\": output_template,\n",
    "    }\n",
    "\n",
    "inspect_model(pm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3389bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [EXAMPLE] How to use shared_data in other functions\n",
    "# ------------------------------------------------------------\n",
    "with pm_model:\n",
    "\n",
    "    # Get shared_data\n",
    "    data_type = pm_model.shared_data[\"data_type\"]\n",
    "    ages      = pm_model.shared_data[\"ages\"]\n",
    "    data      = pm_model.shared_data[\"data\"]\n",
    "    lb_data   = pm_model.shared_data[\"lb_data\"]\n",
    "    knots     = pm_model.shared_data[\"knots\"]\n",
    "    smoothing = pm_model.shared_data[\"smoothing\"]\n",
    "# ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c277825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Dev/AMD/dismod_mr_migrated/reforged_mr/model/spline.py\n",
      "printing type of gamma\n",
      "<class 'list'>\n",
      "[gamma_p_0, gamma_p_1, gamma_p_2, gamma_p_3, gamma_p_4, gamma_p_5]\n",
      "printing type of gamma_vec\n",
      "<class 'pytensor.tensor.var.TensorVariable'>\n",
      "MakeVector{dtype='float64'}.0\n",
      "printing type of exp_gamma\n",
      "<class 'pytensor.tensor.var.TensorVariable'>\n",
      "Elemwise{exp,no_inplace}.0\n",
      "printing type of W_t\n",
      "<class 'pytensor.tensor.var.TensorConstant'>\n",
      "TensorConstant{[[1.      ..        ]]}\n",
      "printing type of mu_age before pm.Deterministic\n",
      "<class 'pytensor.tensor.var.TensorVariable'>\n",
      "dot.0\n",
      "printing type of mu_age after pm.Deterministic\n",
      "<class 'pytensor.tensor.var.TensorVariable'>\n",
      "dot.0\n",
      "📊 Model Summary:\n",
      "  • Free RVs       : 6 ['gamma_p_0', 'gamma_p_1', 'gamma_p_2', 'gamma_p_3', 'gamma_p_4', 'gamma_p_5']\n",
      "  • Observed RVs   : 0 []\n",
      "  • Deterministics : 1 ['mu_age_p']\n",
      "  • Potentials     : 1 ['smooth_p']\n",
      "  • Total Named RVs: 8\n",
      "-----------------------------------------------------\n",
      "🔍 Variable: gamma_p_0\n",
      "  • Type     : <class 'pytensor.tensor.var.TensorVariable'>\n",
      "  • Shape    : TensorConstant{[]}\n",
      "  • DType    : float64\n",
      "  • Owner OP : normal_rv{0, (0, 0), floatX, False}\n",
      "  • Current value (eval): -1.129979566829543\n"
     ]
    }
   ],
   "source": [
    "# process.asr() | (1) spline.py\n",
    "import model.spline as spline\n",
    "print(spline.__file__)\n",
    "\n",
    "with pm_model:\n",
    "    if mu_age is not None:\n",
    "        unconstrained_mu_age_tv = mu_age\n",
    "        \n",
    "    else:\n",
    "        unconstrained_mu_age_tv = spline.spline()\n",
    "        \n",
    "inspect_model(pm_model, show_shared_data=False)\n",
    "print(\"-----------------------------------------------------\")\n",
    "inspect_model(pm_model, var_name='gamma_p_0', show_shared_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "118b1161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Dev/AMD/dismod_mr_migrated/reforged_mr/model/priors.py\n",
      "📊 Model Summary:\n",
      "  • Free RVs       : 6 ['gamma_p_0', 'gamma_p_1', 'gamma_p_2', 'gamma_p_3', 'gamma_p_4', 'gamma_p_5']\n",
      "  • Observed RVs   : 0 []\n",
      "  • Deterministics : 2 ['mu_age_p', 'value_constrained_mu_age_p']\n",
      "  • Potentials     : 2 ['smooth_p', 'parent_similarity_p']\n",
      "  • Total Named RVs: 10\n"
     ]
    }
   ],
   "source": [
    "# process.asr() | (2) priors.py - level_constraints()\n",
    "import model.priors as priors\n",
    "print(priors.__file__)\n",
    "\n",
    "with pm_model:\n",
    "    constrained_mu_age_tv, unconstrained_mu_age_tv, parent_similarity_tv= \\\n",
    "        priors.level_constraints(unconstrained_mu_age_tv)\n",
    "    \n",
    "\n",
    "inspect_model(pm_model, show_shared_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce9b72f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Model Summary:\n",
      "  • Free RVs       : 6 ['gamma_p_0', 'gamma_p_1', 'gamma_p_2', 'gamma_p_3', 'gamma_p_4', 'gamma_p_5']\n",
      "  • Observed RVs   : 0 []\n",
      "  • Deterministics : 2 ['mu_age_p', 'value_constrained_mu_age_p']\n",
      "  • Potentials     : 3 ['smooth_p', 'parent_similarity_p', 'mu_age_derivative_potential_p']\n",
      "  • Total Named RVs: 11\n"
     ]
    }
   ],
   "source": [
    "# process.asr() | (3) priors.py - derivative_constraints()\n",
    "with pm_model:    \n",
    "        mu_age_derivative_tv = priors.derivative_constraints(mu_age=constrained_mu_age_tv)\n",
    "    \n",
    "inspect_model(pm_model, show_shared_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71be6de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Model Summary:\n",
      "  • Free RVs       : 6 ['gamma_p_0', 'gamma_p_1', 'gamma_p_2', 'gamma_p_3', 'gamma_p_4', 'gamma_p_5']\n",
      "  • Observed RVs   : 0 []\n",
      "  • Deterministics : 2 ['mu_age_p', 'value_constrained_mu_age_p']\n",
      "  • Potentials     : 3 ['smooth_p', 'parent_similarity_p', 'mu_age_derivative_potential_p']\n",
      "  • Total Named RVs: 11\n"
     ]
    }
   ],
   "source": [
    "# process.asr() | (4) priors.py - similar()\n",
    "with pm_model:    \n",
    "    if mu_age_parent is not None:\n",
    "        parent_similarity_tv = priors.similar( # TODO: similar() is also used in level_constraints(). \n",
    "            mu_child=mu_age_derivative_tv,     #      Thus, it is hard to reduce parameters.\n",
    "            mu_parent=mu_age_parent,           #      Moreover, concerns on pm.Potential(parent_similarity_tv)\n",
    "            sigma_parent=sigma_age_parent,     #      What happens if it is called twice with same name?\n",
    "            sigma_difference=0.0,\n",
    "            offset=1e-9\n",
    "        )\n",
    "\n",
    "inspect_model(pm_model, show_shared_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0e8c6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Dev/AMD/dismod_mr_migrated/reforged_mr/model/age_groups.py\n",
      "/Users/Dev/AMD/dismod_mr_migrated/reforged_mr/model/covariates.py\n",
      "📊 Model Summary:\n",
      "  • Free RVs       : 46 ['gamma_p_0', 'gamma_p_1', 'gamma_p_2', 'gamma_p_3', 'gamma_p_4', 'gamma_p_5', 'sigma_alpha_p_0_z', 'sigma_alpha_p_1_z', 'sigma_alpha_p_2_z', 'sigma_alpha_p_3_z', 'sigma_alpha_p_4_z', 'alpha_p_31', 'alpha_p_56', 'alpha_p_62', 'alpha_p_64', 'alpha_p_70', 'alpha_p_71', 'alpha_p_65', 'alpha_p_67', 'alpha_p_68', 'alpha_p_69', 'alpha_p_100', 'alpha_p_102', 'alpha_p_73', 'alpha_p_81', 'alpha_p_83', 'alpha_p_84', 'alpha_p_86', 'alpha_p_89', 'alpha_p_92', 'alpha_p_137', 'alpha_p_138', 'alpha_p_142', 'alpha_p_158', 'alpha_p_159', 'alpha_p_163', 'alpha_p_164', 'alpha_p_4', 'alpha_p_5', 'alpha_p_6', 'alpha_p_8', 'alpha_p_9', 'alpha_p_18', 'beta_p_x_sdi', 'beta_p_x_tob', 'beta_p_x_sex']\n",
      "  • Observed RVs   : 0 []\n",
      "  • Deterministics : 10 ['mu_age_p', 'value_constrained_mu_age_p', 'cum_sum_mu_p', 'mu_interval_p', 'sigma_alpha_p_0', 'sigma_alpha_p_1', 'sigma_alpha_p_2', 'sigma_alpha_p_3', 'sigma_alpha_p_4', 'pi_p']\n",
      "  • Potentials     : 8 ['smooth_p', 'parent_similarity_p', 'mu_age_derivative_potential_p', 'sigma_alpha_p_0_trunc', 'sigma_alpha_p_1_trunc', 'sigma_alpha_p_2_trunc', 'sigma_alpha_p_3_trunc', 'sigma_alpha_p_4_trunc']\n",
      "  • Total Named RVs: 64\n"
     ]
    }
   ],
   "source": [
    "# process.asr() | (5) age_groups.py - age_standardize_approx()\n",
    "#               | (6) covariate.py - mean_covariate_model()\n",
    "import model.age_groups as age_groups\n",
    "import model.covariates as covariates\n",
    "print(age_groups.__file__)\n",
    "print(covariates.__file__)\n",
    "\n",
    "with pm_model:    \n",
    "    if len(data) > 0:\n",
    "        data = data.copy()\n",
    "        # 2-1) standard_error, effective_sample_size 채우기\n",
    "        se = data['standard_error'].mask(\n",
    "            data['standard_error'] < 0,\n",
    "            (data['upper_ci'] - data['lower_ci']) / (2 * 1.96)\n",
    "        )\n",
    "        ess = data['effective_sample_size'].fillna(\n",
    "            data['value'] * (1 - data['value']) / se**2\n",
    "        )\n",
    "        data['standard_error'] = se\n",
    "        data['effective_sample_size'] = ess\n",
    "\n",
    "        mu_interval_tv = age_groups.age_standardize_approx(mu_age=mu_age_derivative_tv)\n",
    "\n",
    "        # 2-2) covariate & pi\n",
    "        if include_covariates:\n",
    "            pi_tv = covariates.mean_covariate_model(mu=mu_interval_tv)\n",
    "\n",
    "        else:\n",
    "            pi_tv = mu_interval_tv\n",
    "\n",
    "\n",
    "inspect_model(pm_model, show_shared_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f379c85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Model Summary:\n",
      "  • Free RVs       : 46 ['gamma_p_0', 'gamma_p_1', 'gamma_p_2', 'gamma_p_3', 'gamma_p_4', 'gamma_p_5', 'sigma_alpha_p_0_z', 'sigma_alpha_p_1_z', 'sigma_alpha_p_2_z', 'sigma_alpha_p_3_z', 'sigma_alpha_p_4_z', 'alpha_p_31', 'alpha_p_56', 'alpha_p_62', 'alpha_p_64', 'alpha_p_70', 'alpha_p_71', 'alpha_p_65', 'alpha_p_67', 'alpha_p_68', 'alpha_p_69', 'alpha_p_100', 'alpha_p_102', 'alpha_p_73', 'alpha_p_81', 'alpha_p_83', 'alpha_p_84', 'alpha_p_86', 'alpha_p_89', 'alpha_p_92', 'alpha_p_137', 'alpha_p_138', 'alpha_p_142', 'alpha_p_158', 'alpha_p_159', 'alpha_p_163', 'alpha_p_164', 'alpha_p_4', 'alpha_p_5', 'alpha_p_6', 'alpha_p_8', 'alpha_p_9', 'alpha_p_18', 'beta_p_x_sdi', 'beta_p_x_tob', 'beta_p_x_sex']\n",
      "  • Observed RVs   : 0 []\n",
      "  • Deterministics : 10 ['mu_age_p', 'value_constrained_mu_age_p', 'cum_sum_mu_p', 'mu_interval_p', 'sigma_alpha_p_0', 'sigma_alpha_p_1', 'sigma_alpha_p_2', 'sigma_alpha_p_3', 'sigma_alpha_p_4', 'pi_p']\n",
      "  • Potentials     : 8 ['smooth_p', 'parent_similarity_p', 'mu_age_derivative_potential_p', 'sigma_alpha_p_0_trunc', 'sigma_alpha_p_1_trunc', 'sigma_alpha_p_2_trunc', 'sigma_alpha_p_3_trunc', 'sigma_alpha_p_4_trunc']\n",
      "  • Total Named RVs: 64\n"
     ]
    }
   ],
   "source": [
    "# process.asr() | (7) covariate.py - mean_covariate_model()\n",
    "\n",
    "with pm_model:    \n",
    "    if len(data) <= 0:\n",
    "        if include_covariates:\n",
    "            pi_tv = covariates.mean_covariate_model(mu=None)\n",
    "\n",
    "\n",
    "inspect_model(pm_model, show_shared_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f8513b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_types = ['beta_binom', 'binom', 'neg_binom', 'poisson', 'log_normal', 'normal', 'offset_log_normal']\n",
    "rate_type = rate_types[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c1aa46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Dev/AMD/dismod_mr_migrated/reforged_mr/model/likelihood.py\n",
      "📊 Model Summary:\n",
      "  • Free RVs       : 46 ['gamma_p_0', 'gamma_p_1', 'gamma_p_2', 'gamma_p_3', 'gamma_p_4', 'gamma_p_5', 'sigma_alpha_p_0_z', 'sigma_alpha_p_1_z', 'sigma_alpha_p_2_z', 'sigma_alpha_p_3_z', 'sigma_alpha_p_4_z', 'alpha_p_31', 'alpha_p_56', 'alpha_p_62', 'alpha_p_64', 'alpha_p_70', 'alpha_p_71', 'alpha_p_65', 'alpha_p_67', 'alpha_p_68', 'alpha_p_69', 'alpha_p_100', 'alpha_p_102', 'alpha_p_73', 'alpha_p_81', 'alpha_p_83', 'alpha_p_84', 'alpha_p_86', 'alpha_p_89', 'alpha_p_92', 'alpha_p_137', 'alpha_p_138', 'alpha_p_142', 'alpha_p_158', 'alpha_p_159', 'alpha_p_163', 'alpha_p_164', 'alpha_p_4', 'alpha_p_5', 'alpha_p_6', 'alpha_p_8', 'alpha_p_9', 'alpha_p_18', 'beta_p_x_sdi', 'beta_p_x_tob', 'beta_p_x_sex']\n",
      "  • Observed RVs   : 0 []\n",
      "  • Deterministics : 10 ['mu_age_p', 'value_constrained_mu_age_p', 'cum_sum_mu_p', 'mu_interval_p', 'sigma_alpha_p_0', 'sigma_alpha_p_1', 'sigma_alpha_p_2', 'sigma_alpha_p_3', 'sigma_alpha_p_4', 'pi_p']\n",
      "  • Potentials     : 8 ['smooth_p', 'parent_similarity_p', 'mu_age_derivative_potential_p', 'sigma_alpha_p_0_trunc', 'sigma_alpha_p_1_trunc', 'sigma_alpha_p_2_trunc', 'sigma_alpha_p_3_trunc', 'sigma_alpha_p_4_trunc']\n",
      "  • Total Named RVs: 64\n"
     ]
    }
   ],
   "source": [
    "# process.asr() | (8) covariate.py - dispersion_covariate_model()\n",
    "# process.asr() | (9) likelihood.py - neg_binom()\n",
    "import model.likelihood as likelihood\n",
    "print(likelihood.__file__)\n",
    "\n",
    "with pm_model:\n",
    "    if len(data) > 0:\n",
    "        if rate_type == 'neg_binom':\n",
    "            bad_ess = (data['effective_sample_size'] <= 0) | data['effective_sample_size'].isna()\n",
    "            if bad_ess.any():\n",
    "                data.loc[bad_ess, 'effective_sample_size'] = 0.0\n",
    "\n",
    "            big_ess = data['effective_sample_size'] >= 1e10\n",
    "            if big_ess.any():\n",
    "                data.loc[big_ess, 'effective_sample_size'] = 1e10\n",
    "\n",
    "            hetero = parameters.get('heterogeneity', None)\n",
    "            lower = {'Slightly': 9.0, 'Moderately': 3.0, 'Very': 1.0}.get(hetero, 1.0)\n",
    "            if data_type == 'pf':\n",
    "                lower = 1e12\n",
    "\n",
    "            delta_tv = covariates.dispersion_covariate_model(delta_lb=lower, delta_ub=lower * 9.0)\n",
    "\n",
    "            likelihood.neg_binom(pi=pi_tv, delta=delta_tv)            \n",
    "\n",
    "\n",
    "inspect_model(pm_model, show_shared_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17635268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Model Summary:\n",
      "  • Free RVs       : 46 ['gamma_p_0', 'gamma_p_1', 'gamma_p_2', 'gamma_p_3', 'gamma_p_4', 'gamma_p_5', 'sigma_alpha_p_0_z', 'sigma_alpha_p_1_z', 'sigma_alpha_p_2_z', 'sigma_alpha_p_3_z', 'sigma_alpha_p_4_z', 'alpha_p_31', 'alpha_p_56', 'alpha_p_62', 'alpha_p_64', 'alpha_p_70', 'alpha_p_71', 'alpha_p_65', 'alpha_p_67', 'alpha_p_68', 'alpha_p_69', 'alpha_p_100', 'alpha_p_102', 'alpha_p_73', 'alpha_p_81', 'alpha_p_83', 'alpha_p_84', 'alpha_p_86', 'alpha_p_89', 'alpha_p_92', 'alpha_p_137', 'alpha_p_138', 'alpha_p_142', 'alpha_p_158', 'alpha_p_159', 'alpha_p_163', 'alpha_p_164', 'alpha_p_4', 'alpha_p_5', 'alpha_p_6', 'alpha_p_8', 'alpha_p_9', 'alpha_p_18', 'beta_p_x_sdi', 'beta_p_x_tob', 'beta_p_x_sex']\n",
      "  • Observed RVs   : 0 []\n",
      "  • Deterministics : 10 ['mu_age_p', 'value_constrained_mu_age_p', 'cum_sum_mu_p', 'mu_interval_p', 'sigma_alpha_p_0', 'sigma_alpha_p_1', 'sigma_alpha_p_2', 'sigma_alpha_p_3', 'sigma_alpha_p_4', 'pi_p']\n",
      "  • Potentials     : 8 ['smooth_p', 'parent_similarity_p', 'mu_age_derivative_potential_p', 'sigma_alpha_p_0_trunc', 'sigma_alpha_p_1_trunc', 'sigma_alpha_p_2_trunc', 'sigma_alpha_p_3_trunc', 'sigma_alpha_p_4_trunc']\n",
      "  • Total Named RVs: 64\n"
     ]
    }
   ],
   "source": [
    "# process.asr() | (10) likelihood.py - log_normal()\n",
    "\n",
    "with pm_model:\n",
    "    if len(data) > 0:\n",
    "        if rate_type == 'log_normal':\n",
    "            missing = data['standard_error'] < 0\n",
    "            if missing.any():\n",
    "                data.loc[missing, 'standard_error'] = 1e6\n",
    "\n",
    "            sigma_tv = pm.Uniform(\n",
    "                name=f'sigma_{data_type}',\n",
    "                lower=1e-4,\n",
    "                upper=1.0,\n",
    "            )\n",
    "\n",
    "            likelihood.log_normal(pi=pi_tv, sigma=sigma_tv)\n",
    "\n",
    "\n",
    "inspect_model(pm_model, show_shared_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2b4598",
   "metadata": {},
   "source": [
    "[DEBUG] data_type=p: p 배열에서 0 이하인 값 (총 3개):\n",
    "    index=0, p[0]=[4.09026798e-02 3.38512764e-02 7.23589001e-02 3.69973190e-02\n",
    " 1.03015075e-01 8.70431894e-02 2.04188482e-01 1.37423313e-01\n",
    " 2.46435845e-01 2.40051348e-01 3.14121037e-01 3.22265625e-01\n",
    " 2.66355140e-01 3.56666667e-01 3.88888889e-01 2.16494845e-01\n",
    " 1.41388175e-02 1.23456790e-02 2.70270270e-02 2.33333333e-02\n",
    " 4.30622010e-02 4.27251732e-02 1.04395604e-01 1.13065327e-01\n",
    " 1.68918919e-02 1.83486239e-02 3.16455696e-02 2.49307479e-02\n",
    " 6.45161290e-02 4.60122699e-02 1.28834356e-01 1.36094675e-01\n",
    " 5.05050505e-03 3.96825397e-03 1.89573460e-02 2.76679842e-02\n",
    " 1.30434783e-02 2.77777778e-02 2.38095238e-02 8.73786408e-02\n",
    " 2.70270270e-02 1.42857143e-02 2.61780105e-02 2.18579235e-02\n",
    " 3.63636364e-02 5.34759358e-02 1.14285714e-01 1.20481928e-01\n",
    " 0.00000000e+00 9.80392157e-03 3.12500000e-02 9.70873786e-03\n",
    " 5.00000000e-02 4.95049505e-02 1.48936170e-01 6.97674419e-02\n",
    " 1.67803547e-01 1.61971831e-01 1.36134454e-01 1.31944444e-01\n",
    " 1.66666667e-01 1.26353791e-01 6.95364238e-02 3.88888889e-02\n",
    " 5.69105691e-02 3.79746835e-02 7.69230769e-02 7.81250000e-02\n",
    " 1.18811881e-01 1.83908046e-01 2.33333333e-01 2.85714286e-01\n",
    " 2.85714286e-01 0.00000000e+00 3.42465753e-04 7.11297071e-02\n",
    " 3.11973019e-02 2.87816490e-02 1.56402737e-01 1.60818713e-01\n",
    " 1.69481982e-01 1.28455285e-01 6.93641618e-02 8.09716599e-02\n",
    " 3.79746835e-02 3.30578512e-02 6.97674419e-02 1.34831461e-01\n",
    " 1.55555556e-01 7.90697674e-02 1.52380952e-01 4.54545455e-02\n",
    " 5.69105691e-02 1.37254902e-01 1.47540984e-01 2.19178082e-01\n",
    "...\n",
    " 2.29729730e-01 6.79839577e-03 2.98547390e-02 7.06447188e-02\n",
    " 1.10350982e-01 2.30137091e-02 2.93895112e-02 4.69371519e-02\n",
    " 8.76531574e-02 1.32307692e-01 1.70212766e-01 8.61553785e-02\n",
    " 1.98497854e-01 1.60000699e-02 1.60001603e-02]\n",
    "Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n",
    "---------------------------------------------------------------------------\n",
    "AssertionError                            Traceback (most recent call last)\n",
    "Cell In[47], line 16\n",
    "      8                 data.loc[missing, 'standard_error'] = 1e6\n",
    "     10             sigma_tv = pm.Uniform(\n",
    "     11                 name=f'sigma_{data_type}',\n",
    "     12                 lower=1e-4,\n",
    "     13                 upper=1.0,\n",
    "     14             )\n",
    "---> 16             likelihood.log_normal(pi=pi_tv, sigma=sigma_tv)\n",
    "     19 inspect_model(pm_model, show_shared_data=False)\n",
    "\n",
    "File /Users/Dev/AMD/dismod_mr_migrated/reforged_mr/model/likelihood.py:378, in log_normal(pi, sigma)\n",
    "    375         print(f\"    index={i}, s[{i}]={s[i]}\")\n",
    "    377 # 2) 관측값 유효성 검사\n",
    "--> 378 assert np.all(p > 0), 'observed values must be positive'\n",
    "    379 assert np.all(s >= 0), 'standard error must be non-negative'\n",
    "    381 # 3) 관측 로그값은 NumPy로 미리 계산\n",
    "\n",
    "AssertionError: observed values must be positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2580212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Model Summary:\n",
      "  • Free RVs       : 46 ['gamma_p_0', 'gamma_p_1', 'gamma_p_2', 'gamma_p_3', 'gamma_p_4', 'gamma_p_5', 'sigma_alpha_p_0_z', 'sigma_alpha_p_1_z', 'sigma_alpha_p_2_z', 'sigma_alpha_p_3_z', 'sigma_alpha_p_4_z', 'alpha_p_31', 'alpha_p_56', 'alpha_p_62', 'alpha_p_64', 'alpha_p_70', 'alpha_p_71', 'alpha_p_65', 'alpha_p_67', 'alpha_p_68', 'alpha_p_69', 'alpha_p_100', 'alpha_p_102', 'alpha_p_73', 'alpha_p_81', 'alpha_p_83', 'alpha_p_84', 'alpha_p_86', 'alpha_p_89', 'alpha_p_92', 'alpha_p_137', 'alpha_p_138', 'alpha_p_142', 'alpha_p_158', 'alpha_p_159', 'alpha_p_163', 'alpha_p_164', 'alpha_p_4', 'alpha_p_5', 'alpha_p_6', 'alpha_p_8', 'alpha_p_9', 'alpha_p_18', 'beta_p_x_sdi', 'beta_p_x_tob', 'beta_p_x_sex']\n",
      "  • Observed RVs   : 0 []\n",
      "  • Deterministics : 10 ['mu_age_p', 'value_constrained_mu_age_p', 'cum_sum_mu_p', 'mu_interval_p', 'sigma_alpha_p_0', 'sigma_alpha_p_1', 'sigma_alpha_p_2', 'sigma_alpha_p_3', 'sigma_alpha_p_4', 'pi_p']\n",
      "  • Potentials     : 8 ['smooth_p', 'parent_similarity_p', 'mu_age_derivative_potential_p', 'sigma_alpha_p_0_trunc', 'sigma_alpha_p_1_trunc', 'sigma_alpha_p_2_trunc', 'sigma_alpha_p_3_trunc', 'sigma_alpha_p_4_trunc']\n",
      "  • Total Named RVs: 64\n"
     ]
    }
   ],
   "source": [
    "# process.asr() | (11) likelihood.py - log_normal()\n",
    "\n",
    "with pm_model:\n",
    "    if len(data) > 0:\n",
    "        if rate_type == 'normal':\n",
    "            missing = data['standard_error'] < 0\n",
    "            if missing.any():\n",
    "                data.loc[missing, 'standard_error'] = 1e6\n",
    "\n",
    "            sigma_tv = pm.Uniform(\n",
    "                name=f'sigma_{data_type}',\n",
    "                lower=1e-4,\n",
    "                upper=1e-1,\n",
    "                initval=1e-2\n",
    "            )\n",
    "\n",
    "            likelihood.normal(pi=pi_tv, sigma=sigma_tv)\n",
    "\n",
    "\n",
    "inspect_model(pm_model, show_shared_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fa551a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Model Summary:\n",
      "  • Free RVs       : 46 ['gamma_p_0', 'gamma_p_1', 'gamma_p_2', 'gamma_p_3', 'gamma_p_4', 'gamma_p_5', 'sigma_alpha_p_0_z', 'sigma_alpha_p_1_z', 'sigma_alpha_p_2_z', 'sigma_alpha_p_3_z', 'sigma_alpha_p_4_z', 'alpha_p_31', 'alpha_p_56', 'alpha_p_62', 'alpha_p_64', 'alpha_p_70', 'alpha_p_71', 'alpha_p_65', 'alpha_p_67', 'alpha_p_68', 'alpha_p_69', 'alpha_p_100', 'alpha_p_102', 'alpha_p_73', 'alpha_p_81', 'alpha_p_83', 'alpha_p_84', 'alpha_p_86', 'alpha_p_89', 'alpha_p_92', 'alpha_p_137', 'alpha_p_138', 'alpha_p_142', 'alpha_p_158', 'alpha_p_159', 'alpha_p_163', 'alpha_p_164', 'alpha_p_4', 'alpha_p_5', 'alpha_p_6', 'alpha_p_8', 'alpha_p_9', 'alpha_p_18', 'beta_p_x_sdi', 'beta_p_x_tob', 'beta_p_x_sex']\n",
      "  • Observed RVs   : 0 []\n",
      "  • Deterministics : 10 ['mu_age_p', 'value_constrained_mu_age_p', 'cum_sum_mu_p', 'mu_interval_p', 'sigma_alpha_p_0', 'sigma_alpha_p_1', 'sigma_alpha_p_2', 'sigma_alpha_p_3', 'sigma_alpha_p_4', 'pi_p']\n",
      "  • Potentials     : 8 ['smooth_p', 'parent_similarity_p', 'mu_age_derivative_potential_p', 'sigma_alpha_p_0_trunc', 'sigma_alpha_p_1_trunc', 'sigma_alpha_p_2_trunc', 'sigma_alpha_p_3_trunc', 'sigma_alpha_p_4_trunc']\n",
      "  • Total Named RVs: 64\n"
     ]
    }
   ],
   "source": [
    "# process.asr() | (12) likelihood.py - binom()\n",
    "\n",
    "with pm_model:\n",
    "    if len(data) > 0:\n",
    "        if rate_type == 'binom':\n",
    "            bad_ess = data['effective_sample_size'] < 0\n",
    "            if bad_ess.any():\n",
    "                data.loc[bad_ess, 'effective_sample_size'] = 0.0\n",
    "\n",
    "            likelihood.binom(pi=pi_tv)\n",
    "\n",
    "\n",
    "inspect_model(pm_model, show_shared_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32a50478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Model Summary:\n",
      "  • Free RVs       : 46 ['gamma_p_0', 'gamma_p_1', 'gamma_p_2', 'gamma_p_3', 'gamma_p_4', 'gamma_p_5', 'sigma_alpha_p_0_z', 'sigma_alpha_p_1_z', 'sigma_alpha_p_2_z', 'sigma_alpha_p_3_z', 'sigma_alpha_p_4_z', 'alpha_p_31', 'alpha_p_56', 'alpha_p_62', 'alpha_p_64', 'alpha_p_70', 'alpha_p_71', 'alpha_p_65', 'alpha_p_67', 'alpha_p_68', 'alpha_p_69', 'alpha_p_100', 'alpha_p_102', 'alpha_p_73', 'alpha_p_81', 'alpha_p_83', 'alpha_p_84', 'alpha_p_86', 'alpha_p_89', 'alpha_p_92', 'alpha_p_137', 'alpha_p_138', 'alpha_p_142', 'alpha_p_158', 'alpha_p_159', 'alpha_p_163', 'alpha_p_164', 'alpha_p_4', 'alpha_p_5', 'alpha_p_6', 'alpha_p_8', 'alpha_p_9', 'alpha_p_18', 'beta_p_x_sdi', 'beta_p_x_tob', 'beta_p_x_sex']\n",
      "  • Observed RVs   : 0 []\n",
      "  • Deterministics : 10 ['mu_age_p', 'value_constrained_mu_age_p', 'cum_sum_mu_p', 'mu_interval_p', 'sigma_alpha_p_0', 'sigma_alpha_p_1', 'sigma_alpha_p_2', 'sigma_alpha_p_3', 'sigma_alpha_p_4', 'pi_p']\n",
      "  • Potentials     : 8 ['smooth_p', 'parent_similarity_p', 'mu_age_derivative_potential_p', 'sigma_alpha_p_0_trunc', 'sigma_alpha_p_1_trunc', 'sigma_alpha_p_2_trunc', 'sigma_alpha_p_3_trunc', 'sigma_alpha_p_4_trunc']\n",
      "  • Total Named RVs: 64\n"
     ]
    }
   ],
   "source": [
    "# process.asr() | (13) likelihood.py - beta_binom()\n",
    "\n",
    "with pm_model:\n",
    "    if len(data) > 0:\n",
    "        if rate_type == 'beta_binom':\n",
    "\n",
    "            ### NEWLY ADDED: Origianl code doesn't have delta_tv calculation\n",
    "            bad_ess = (data['effective_sample_size'] <= 0) | data['effective_sample_size'].isna()\n",
    "            if bad_ess.any():\n",
    "                data.loc[bad_ess, 'effective_sample_size'] = 0.0\n",
    "\n",
    "            big_ess = data['effective_sample_size'] >= 1e10\n",
    "            if big_ess.any():\n",
    "                data.loc[big_ess, 'effective_sample_size'] = 1e10\n",
    "\n",
    "            hetero = parameters.get('heterogeneity', None)\n",
    "            lower = {'Slightly': 9.0, 'Moderately': 3.0, 'Very': 1.0}.get(hetero, 1.0)\n",
    "            if data_type == 'pf':\n",
    "                lower = 1e12\n",
    "\n",
    "            delta_tv = covariates.dispersion_covariate_model(delta_lb=lower, delta_ub=lower * 9.0)\n",
    "\n",
    "            likelihood.beta_binom(pi=pi_tv, delta=delta_tv)\n",
    "\n",
    "\n",
    "inspect_model(pm_model, show_shared_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545ff40e",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "IndexError                                Traceback (most recent call last)\n",
    "Cell In[25], line 23\n",
    "     19                 lower = 1e12\n",
    "     21             delta_tv = covariates.dispersion_covariate_model(delta_lb=lower, delta_ub=lower * 9.0)\n",
    "---> 23             likelihood.beta_binom(pi=pi_tv, delta=delta_tv)\n",
    "     26 inspect_model(pm_model, show_shared_data=False)\n",
    "\n",
    "File /Users/Dev/AMD/dismod_mr_migrated/reforged_mr/model/likelihood.py:275, in beta_binom(pi, delta)\n",
    "    267 alpha_param = pi * delta * 50\n",
    "    268 beta_param = (1 - pi) * delta * 50\n",
    "    270 p_obs = pm.BetaBinomial(\n",
    "    271     name=f'p_obs_{data_type}',\n",
    "    272     n=n_int[mask],\n",
    "    273     alpha=alpha_param[mask] if hasattr(alpha_param, 'shape') else alpha_param,\n",
    "    274     beta=beta_param[mask] if hasattr(beta_param, 'shape') else beta_param,\n",
    "--> 275     observed=obs_counts[mask]\n",
    "    276 )\n",
    "    278 # Posterior predictive counts: replace zero-sample cases\n",
    "    279 n_pred = n_int.copy()\n",
    "\n",
    "IndexError: boolean index did not match indexed array along dimension 0; dimension is 1 but corresponding boolean dimension is 207"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2926328c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Model Summary:\n",
      "  • Free RVs       : 46 ['gamma_p_0', 'gamma_p_1', 'gamma_p_2', 'gamma_p_3', 'gamma_p_4', 'gamma_p_5', 'sigma_alpha_p_0_z', 'sigma_alpha_p_1_z', 'sigma_alpha_p_2_z', 'sigma_alpha_p_3_z', 'sigma_alpha_p_4_z', 'alpha_p_31', 'alpha_p_56', 'alpha_p_62', 'alpha_p_64', 'alpha_p_70', 'alpha_p_71', 'alpha_p_65', 'alpha_p_67', 'alpha_p_68', 'alpha_p_69', 'alpha_p_100', 'alpha_p_102', 'alpha_p_73', 'alpha_p_81', 'alpha_p_83', 'alpha_p_84', 'alpha_p_86', 'alpha_p_89', 'alpha_p_92', 'alpha_p_137', 'alpha_p_138', 'alpha_p_142', 'alpha_p_158', 'alpha_p_159', 'alpha_p_163', 'alpha_p_164', 'alpha_p_4', 'alpha_p_5', 'alpha_p_6', 'alpha_p_8', 'alpha_p_9', 'alpha_p_18', 'beta_p_x_sdi', 'beta_p_x_tob', 'beta_p_x_sex']\n",
      "  • Observed RVs   : 0 []\n",
      "  • Deterministics : 10 ['mu_age_p', 'value_constrained_mu_age_p', 'cum_sum_mu_p', 'mu_interval_p', 'sigma_alpha_p_0', 'sigma_alpha_p_1', 'sigma_alpha_p_2', 'sigma_alpha_p_3', 'sigma_alpha_p_4', 'pi_p']\n",
      "  • Potentials     : 8 ['smooth_p', 'parent_similarity_p', 'mu_age_derivative_potential_p', 'sigma_alpha_p_0_trunc', 'sigma_alpha_p_1_trunc', 'sigma_alpha_p_2_trunc', 'sigma_alpha_p_3_trunc', 'sigma_alpha_p_4_trunc']\n",
      "  • Total Named RVs: 64\n"
     ]
    }
   ],
   "source": [
    "# process.asr() | (14) likelihood.py - poisson()\n",
    "\n",
    "with pm_model:\n",
    "    if len(data) > 0:\n",
    "        if rate_type == 'poisson':\n",
    "            bad_ess = data['effective_sample_size'] < 0\n",
    "            if bad_ess.any():\n",
    "                data.loc[bad_ess, 'effective_sample_size'] = 0.0\n",
    "\n",
    "            likelihood.poisson(pi=pi_tv)\n",
    "\n",
    "\n",
    "inspect_model(pm_model, show_shared_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b033021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Model Summary:\n",
      "  • Free RVs       : 49 ['gamma_p_0', 'gamma_p_1', 'gamma_p_2', 'gamma_p_3', 'gamma_p_4', 'gamma_p_5', 'sigma_alpha_p_0_z', 'sigma_alpha_p_1_z', 'sigma_alpha_p_2_z', 'sigma_alpha_p_3_z', 'sigma_alpha_p_4_z', 'alpha_p_31', 'alpha_p_56', 'alpha_p_62', 'alpha_p_64', 'alpha_p_70', 'alpha_p_71', 'alpha_p_65', 'alpha_p_67', 'alpha_p_68', 'alpha_p_69', 'alpha_p_100', 'alpha_p_102', 'alpha_p_73', 'alpha_p_81', 'alpha_p_83', 'alpha_p_84', 'alpha_p_86', 'alpha_p_89', 'alpha_p_92', 'alpha_p_137', 'alpha_p_138', 'alpha_p_142', 'alpha_p_158', 'alpha_p_159', 'alpha_p_163', 'alpha_p_164', 'alpha_p_4', 'alpha_p_5', 'alpha_p_6', 'alpha_p_8', 'alpha_p_9', 'alpha_p_18', 'beta_p_x_sdi', 'beta_p_x_tob', 'beta_p_x_sex', 'sigma_p', 'p_zeta_p', 'log_pred_p']\n",
      "  • Observed RVs   : 0 []\n",
      "  • Deterministics : 11 ['mu_age_p', 'value_constrained_mu_age_p', 'cum_sum_mu_p', 'mu_interval_p', 'sigma_alpha_p_0', 'sigma_alpha_p_1', 'sigma_alpha_p_2', 'sigma_alpha_p_3', 'sigma_alpha_p_4', 'pi_p', 'p_pred_p']\n",
      "  • Potentials     : 9 ['smooth_p', 'parent_similarity_p', 'mu_age_derivative_potential_p', 'sigma_alpha_p_0_trunc', 'sigma_alpha_p_1_trunc', 'sigma_alpha_p_2_trunc', 'sigma_alpha_p_3_trunc', 'sigma_alpha_p_4_trunc', 'p_obs_likelihood_p']\n",
      "  • Total Named RVs: 69\n"
     ]
    }
   ],
   "source": [
    "# process.asr() | (15) likelihood.py - offset_log_normal()\n",
    "#               | (16) else\n",
    "\n",
    "with pm_model:\n",
    "    if len(data) > 0:\n",
    "        if rate_type == 'offset_log_normal':\n",
    "            \n",
    "            sigma_tv = pm.Uniform(\n",
    "                name=f'sigma_{data_type}',\n",
    "                lower=1e-4,\n",
    "                upper=10.0,\n",
    "                initval=1e-2\n",
    "            )\n",
    "\n",
    "            likelihood.offset_log_normal(pi=pi_tv, sigma=sigma_tv)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported rate_type \"{rate_type}\"')\n",
    "\n",
    "\n",
    "inspect_model(pm_model, show_shared_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d253fd4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ed4a9693",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Level 3: country\u001b[39;00m\n\u001b[1;32m      2\u001b[0m countries \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mhier\u001b[49m[hier[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLevel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLocation ID\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc3_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLocation Name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParent ID\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparent2_id\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m     })\n\u001b[1;32m      9\u001b[0m     [[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc3_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparent2_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Level 2: region\u001b[39;00m\n\u001b[1;32m     13\u001b[0m regions \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     14\u001b[0m     hier[hier[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLevel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     [[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc2_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparent1_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     21\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hier' is not defined"
     ]
    }
   ],
   "source": [
    "# Level 3: country\n",
    "countries = (\n",
    "    hier[hier[\"Level\"] == 3]\n",
    "    .rename(columns={\n",
    "        \"Location ID\": \"loc3_id\",\n",
    "        \"Location Name\": \"country\",\n",
    "        \"Parent ID\": \"parent2_id\"\n",
    "    })\n",
    "    [[\"loc3_id\", \"country\", \"parent2_id\"]]\n",
    ")\n",
    "\n",
    "# Level 2: region\n",
    "regions = (\n",
    "    hier[hier[\"Level\"] == 2]\n",
    "    .rename(columns={\n",
    "        \"Location ID\": \"loc2_id\",\n",
    "        \"Location Name\": \"region\",\n",
    "        \"Parent ID\": \"parent1_id\"\n",
    "    })\n",
    "    [[\"loc2_id\", \"region\", \"parent1_id\"]]\n",
    ")\n",
    "\n",
    "# Level 1: super-region\n",
    "superregs = (\n",
    "    hier[hier[\"Level\"] == 1]\n",
    "    .rename(columns={\n",
    "        \"Location ID\": \"loc1_id\",\n",
    "        \"Location Name\": \"super_region\"\n",
    "    })\n",
    "    [[\"loc1_id\", \"super_region\"]]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc9e0ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>super_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>High-income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States of America</td>\n",
       "      <td>High-income North America</td>\n",
       "      <td>High-income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Israel</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>High-income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>High-income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Canada</td>\n",
       "      <td>High-income North America</td>\n",
       "      <td>High-income</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    country                     region super_region\n",
       "0                    Sweden             Western Europe  High-income\n",
       "1  United States of America  High-income North America  High-income\n",
       "3                    Israel             Western Europe  High-income\n",
       "5               Switzerland             Western Europe  High-income\n",
       "7                    Canada  High-income North America  High-income"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.1 country 정보 붙이기\n",
    "df = df.merge(countries, on=\"country\", how=\"left\")\n",
    "\n",
    "# 4.2 region 정보 붙이기\n",
    "df = df.merge(\n",
    "    regions,\n",
    "    left_on=\"parent2_id\",\n",
    "    right_on=\"loc2_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 4.3 super-region 정보 붙이기\n",
    "df = df.merge(\n",
    "    superregs,\n",
    "    left_on=\"parent1_id\",\n",
    "    right_on=\"loc1_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 확인\n",
    "df[[\"country\",\"region\",\"super_region\"]].drop_duplicates().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6de9264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     study                   country  \\\n",
      "0   Nilsson LV et al, 1984                    Sweden   \n",
      "1     Kramer M et al, 1985  United States of America   \n",
      "2    Flament M et al, 1990  United States of America   \n",
      "3     Zohar AH et al, 1992                    Israel   \n",
      "4  Reinherz HZ et al, 1993  United States of America   \n",
      "\n",
      "                      region super_region  age_code  meth_code  prev_code  \\\n",
      "0             Western Europe  High-income         1          0          0   \n",
      "1  High-income North America  High-income         1          0          0   \n",
      "2  High-income North America  High-income         1          0          0   \n",
      "3             Western Europe  High-income         0          0          0   \n",
      "4  High-income North America  High-income         0          0          0   \n",
      "\n",
      "   prevalence_value  \n",
      "0              98.0  \n",
      "1              22.0  \n",
      "2              55.0  \n",
      "3              36.0  \n",
      "4              21.0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2) 매핑 딕셔너리\n",
    "age_map  = {\"Children\":0, \"Adult\":1, \"Both\":2}\n",
    "meth_map = {\n",
    "    \"Interview based\":0,\n",
    "    \"Non-interview\":1\n",
    "}\n",
    "prev_map = {\n",
    "    \"point prevalence\":     0,\n",
    "    \"12-month prevalence\":  1,\n",
    "    \"life-time prevalence\": 2\n",
    "}\n",
    "\n",
    "# 3) long 포맷으로 전환\n",
    "df_long = df.melt(\n",
    "    id_vars=[\n",
    "        \"study\",             \n",
    "        \"country\",\n",
    "        \"region\",\n",
    "        \"super_region\",\n",
    "        \"Methods of questionnaire\",\n",
    "        \"Age range\",\n",
    "        \"study population\"\n",
    "    ],\n",
    "    value_vars=[\n",
    "        \"point prevalence\",\n",
    "        \"12-month prevalence\",\n",
    "        \"life-time prevalence\"\n",
    "    ],\n",
    "    var_name=\"prevalence_type\",\n",
    "    value_name=\"prevalence_value\"\n",
    ").dropna(subset=[\"prevalence_value\"])\n",
    "\n",
    "# 4) 코드 매핑\n",
    "df_long[\"age_code\"]  = df_long[\"Age range\"].map(age_map)\n",
    "df_long[\"meth_code\"] = df_long[\"Methods of questionnaire\"].map(meth_map)\n",
    "df_long[\"prev_code\"] = df_long[\"prevalence_type\"].map(prev_map)\n",
    "\n",
    "# 5) 최종 확인\n",
    "result = df_long[[\n",
    "    \"study\", \"country\", \"region\", \"super_region\",\n",
    "    \"age_code\", \"meth_code\",\n",
    "    \"prev_code\", \"prevalence_value\"\n",
    "]]\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca0029bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) 비율(p_hat) 및 로그 변환\n",
    "#    prevalence_value: point/12-month/life-time 케이스 수\n",
    "#    study population: 전체 표본 수\n",
    "\n",
    "# 6.1 p_hat 계산\n",
    "df_long[\"p_hat\"] = df_long[\"prevalence_value\"] / df_long[\"study population\"]\n",
    "\n",
    "# 6.2 로그 변환 (필요 시 0 회피를 위해 clamp)\n",
    "eps = 1e-6\n",
    "df_long[\"p_hat\"] = df_long[\"p_hat\"].clip(eps, 1 - eps)\n",
    "df_long[\"log_p_hat\"] = np.log(df_long[\"p_hat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe5e535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study</th>\n",
       "      <th>prevalence_type</th>\n",
       "      <th>multi_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nilsson LV et al, 1984</td>\n",
       "      <td>point prevalence</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kramer M et al, 1985</td>\n",
       "      <td>point prevalence</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flament M et al, 1990</td>\n",
       "      <td>point prevalence</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zohar AH et al, 1992</td>\n",
       "      <td>point prevalence</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reinherz HZ et al, 1993</td>\n",
       "      <td>point prevalence</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     study   prevalence_type  multi_mask\n",
       "0   Nilsson LV et al, 1984  point prevalence           0\n",
       "1     Kramer M et al, 1985  point prevalence           0\n",
       "2    Flament M et al, 1990  point prevalence           1\n",
       "3     Zohar AH et al, 1992  point prevalence           0\n",
       "4  Reinherz HZ et al, 1993  point prevalence           0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) df_long 이용: prevalence_type 당 개수 세기\n",
    "prev_count = df_long.groupby(\"study\")[\"prevalence_type\"] \\\n",
    "                    .transform(\"nunique\")\n",
    "\n",
    "# 2) 2개 이상이면 1, 아니면 0\n",
    "df_long[\"multi_mask\"] = (prev_count >= 2).astype(int)\n",
    "\n",
    "# 3) 확인\n",
    "df_long[[\"study\",\"prevalence_type\",\"multi_mask\"]].drop_duplicates().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ecf578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) hierarchy 에서 좌표(리스트) 직접 추출\n",
    "# level 1 → super_region (7개)\n",
    "super_list = (\n",
    "    hier[hier[\"Level\"] == 1]\n",
    "    [\"Location Name\"]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# level 2 → region (21개)\n",
    "region_list = (\n",
    "    hier[hier[\"Level\"] == 2]\n",
    "    [\"Location Name\"]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# level 3 → country (180+개)\n",
    "country_list = (\n",
    "    hier[hier[\"Level\"] == 3]\n",
    "    [\"Location Name\"]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# 2) coords 정의 (unchanged)\n",
    "coords = {\n",
    "    \"country\":      country_list,\n",
    "    \"region\":       region_list,\n",
    "    \"super_region\": super_list,\n",
    "    \"age\":          [\"Children\", \"Adult\", \"Both\"],\n",
    "    \"meth\":         [\"Interview based\", \"Non-interview\"],\n",
    "    \"prev\":         [\"point prevalence\", \"12-month prevalence\", \"life-time prevalence\"],\n",
    "    \"study\":        df_long[\"study\"].unique().tolist()\n",
    "}\n",
    "\n",
    "# 3) questionnaire method 컬럼명 변경 반영\n",
    "#    이미 엑셀에서 \"Methods of questionnaire\" 아래 값을 \"Interview based\" / \"Non-interview\"로 바꿔 두셨다면\n",
    "meth_map = {\n",
    "    \"Interview based\":    0,\n",
    "    \"Non-interview\":      1\n",
    "}\n",
    "df_long[\"meth_code\"] = df_long[\"Methods of questionnaire\"].map(meth_map)\n",
    "\n",
    "# 4) index 배열 생성\n",
    "country_idx = pd.Categorical(\n",
    "    df_long[\"country\"],\n",
    "    categories=coords[\"country\"]\n",
    ").codes\n",
    "\n",
    "region_idx = pd.Categorical(\n",
    "    df_long[\"region\"],\n",
    "    categories=coords[\"region\"]\n",
    ").codes\n",
    "\n",
    "super_idx = pd.Categorical(\n",
    "    df_long[\"super_region\"],\n",
    "    categories=coords[\"super_region\"]\n",
    ").codes\n",
    "\n",
    "age_idx   = df_long[\"age_code\"].to_numpy()\n",
    "meth_idx  = df_long[\"meth_code\"].to_numpy()         \n",
    "prev_idx  = df_long[\"prev_code\"].to_numpy()\n",
    "\n",
    "study_idx = pd.Categorical(\n",
    "    df_long[\"study\"],\n",
    "    categories=coords[\"study\"]\n",
    ").codes\n",
    "\n",
    "# 5) mask / 관측치 벡터 (unchanged)\n",
    "multi_mask = df_long[\"multi_mask\"].to_numpy()\n",
    "y_obs      = df_long[\"log_p_hat\"].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203c1bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa9cf9e5a5442c7ad3bebb3b8c6c0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished [100%]: Average Loss = 395.52\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [β0, β_age, β_meth, β_prev, σ_sr, η_sr, σ_r, η_r, σ_c, η_c, σ_prev_study, η_prev_study, σ_y]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b22a72e62f74df98ebde817413195c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 7188 seconds.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "plot_forest() got an unexpected keyword argument 'credible_interval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# ─── forest plot 으로 깔끔하게 시각화 ───\u001b[39;00m\n\u001b[0;32m     57\u001b[0m az\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marviz-darkgrid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m \u001b[43maz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_forest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43midata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvar_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mβ_age\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mβ_meth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mβ_prev\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mσ_sr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mσ_r\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mσ_c\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mσ_prev_study\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mσ_y\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombined\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcredible_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mridgeplot_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\n\u001b[0;32m     64\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: plot_forest() got an unexpected keyword argument 'credible_interval'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with pm.Model(coords=coords) as model:\n",
    "\n",
    "    # ─── Fixed effects ───\n",
    "    β0    = pm.Normal(\"β0\",    mu=0, sigma=10)\n",
    "    β_age = pm.Normal(\"β_age\", mu=0, sigma=5, dims=\"age\")\n",
    "    β_meth= pm.Normal(\"β_meth\",mu=0, sigma=5, dims=\"meth\")\n",
    "    β_prev= pm.Normal(\"β_prev\",mu=0, sigma=5, dims=\"prev\")\n",
    "\n",
    "    # ─── Non-centered random intercepts ───\n",
    "    σ_sr = pm.HalfNormal(\"σ_sr\", sigma=5)\n",
    "    η_sr = pm.Normal(\"η_sr\", mu=0, sigma=1, dims=\"super_region\")\n",
    "    u_sr = pm.Deterministic(\"u_sr\", η_sr * σ_sr)\n",
    "\n",
    "    σ_r  = pm.HalfNormal(\"σ_r\", sigma=5)\n",
    "    η_r  = pm.Normal(\"η_r\",  mu=0, sigma=1, dims=\"region\")\n",
    "    u_r  = pm.Deterministic(\"u_r\", η_r * σ_r)\n",
    "\n",
    "    σ_c  = pm.HalfNormal(\"σ_c\", sigma=5)\n",
    "    η_c  = pm.Normal(\"η_c\",  mu=0, sigma=1, dims=\"country\")\n",
    "    u_c  = pm.Deterministic(\"u_c\", η_c * σ_c)\n",
    "\n",
    "    # ─── Study-level random slope (non-centered) ───\n",
    "    σ_ps= pm.HalfNormal(\"σ_prev_study\", sigma=5)\n",
    "    η_ps= pm.Normal(\"η_prev_study\", mu=0, sigma=1, dims=(\"study\",\"prev\"))\n",
    "    u_ps= pm.Deterministic(\"u_prev_study\", η_ps * σ_ps)\n",
    "\n",
    "    # ─── 선형 예측식 μ ───\n",
    "    prev_effect = (β_prev[prev_idx] + u_ps[study_idx,prev_idx]) * multi_mask\n",
    "\n",
    "    μ = (\n",
    "        β0\n",
    "        + β_age[age_idx]\n",
    "        + β_meth[meth_idx]\n",
    "        + prev_effect\n",
    "        + u_sr[super_idx]\n",
    "        + u_r[region_idx]\n",
    "        + u_c[country_idx]\n",
    "    )\n",
    "\n",
    "    # ─── 관측모형 ───\n",
    "    σ_y = pm.HalfNormal(\"σ_y\", sigma=1)\n",
    "    pm.Normal(\"y\", mu=μ, sigma=σ_y, observed=y_obs)\n",
    "\n",
    "    # ─── (옵션) ADVI 워밍업 — 빠르게 근사 posterior 생성 ───\n",
    "    advi = pm.fit(method=\"advi\", n=5000)\n",
    "\n",
    "    # ─── NUTS 샘플링 ───\n",
    "    idata = pm.sample(\n",
    "        draws=1000, tune=500,\n",
    "        chains=4, cores=4,\n",
    "        target_accept=0.95,\n",
    "        max_treedepth=15,\n",
    "        progressbar=\"split\"\n",
    "    )\n",
    "\n",
    "# ─── forest plot 으로 깔끔하게 시각화 ───\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "az.plot_forest(\n",
    "    idata,\n",
    "    var_names=[\"β_age\",\"β_meth\",\"β_prev\",\"σ_sr\",\"σ_r\",\"σ_c\",\"σ_prev_study\",\"σ_y\"],\n",
    "    combined=True,\n",
    "    credible_interval=0.95,\n",
    "    ridgeplot_overlap=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76eca402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               mean     sd  hdi_2.5%  hdi_97.5%  mcse_mean  \\\n",
      "β0                           -3.537  4.085   -11.136      4.469      0.111   \n",
      "β_age[Children]              -0.263  2.759    -5.733      4.756      0.080   \n",
      "β_age[Adult]                 -0.284  2.752    -5.538      4.946      0.080   \n",
      "β_age[Both]                  -0.183  2.763    -5.575      4.984      0.080   \n",
      "β_meth[Interview based]       0.241  3.283    -6.431      6.356      0.080   \n",
      "β_meth[Non-interview]        -0.924  3.280    -7.457      5.359      0.080   \n",
      "β_prev[point prevalence]     -1.014  0.388    -1.760     -0.244      0.006   \n",
      "β_prev[12-month prevalence]  -0.401  0.291    -0.957      0.177      0.006   \n",
      "β_prev[life-time prevalence] -0.121  0.290    -0.680      0.434      0.006   \n",
      "σ_sr                          0.916  0.575     0.008      1.955      0.017   \n",
      "σ_r                           0.291  0.242     0.000      0.755      0.008   \n",
      "σ_c                           0.455  0.218     0.011      0.816      0.010   \n",
      "σ_prev_study                  0.182  0.139     0.000      0.449      0.003   \n",
      "σ_y                           0.881  0.080     0.731      1.039      0.002   \n",
      "\n",
      "                              mcse_sd  ess_bulk  ess_tail  r_hat  \n",
      "β0                              0.059  1349.819  2156.025  1.001  \n",
      "β_age[Children]                 0.042  1195.568  1998.628  1.002  \n",
      "β_age[Adult]                    0.042  1198.977  1947.445  1.002  \n",
      "β_age[Both]                     0.042  1187.404  1955.283  1.003  \n",
      "β_meth[Interview based]         0.047  1672.714  2359.159  1.001  \n",
      "β_meth[Non-interview]           0.047  1664.701  2388.832  1.002  \n",
      "β_prev[point prevalence]        0.006  3568.001  2825.607  1.001  \n",
      "β_prev[12-month prevalence]     0.004  2734.519  2586.475  1.001  \n",
      "β_prev[life-time prevalence]    0.004  2495.347  2760.945  1.001  \n",
      "σ_sr                            0.016   851.519   857.021  1.007  \n",
      "σ_r                             0.007  1035.500  1252.875  1.007  \n",
      "σ_c                             0.004   501.880   695.262  1.008  \n",
      "σ_prev_study                    0.002  1607.233  1519.844  1.001  \n",
      "σ_y                             0.001  1135.781  2415.758  1.002  \n"
     ]
    }
   ],
   "source": [
    "import arviz as az\n",
    "\n",
    "# idata는 이미 pm.sample()의 결과\n",
    "summary_df = az.summary(\n",
    "    idata,\n",
    "    var_names=[\n",
    "        \"β0\", \"β_age\", \"β_meth\", \"β_prev\",\n",
    "        \"σ_sr\", \"σ_r\", \"σ_c\", \"σ_prev_study\", \"σ_y\"\n",
    "    ],\n",
    "    round_to=3,         # 소수점 자리수\n",
    "    hdi_prob=0.95       # 95% 신뢰구간\n",
    ")\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a21a0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to C:/Users/정이든/Desktop/연구실/9. prevalence of OCD/3차 와꾸/life_time_prevalence.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 1) posterior를 (chain, draw) → sample 차원 하나로 합치기\n",
    "post = idata.posterior.stack(sample=(\"chain\",\"draw\"))\n",
    "\n",
    "# 2) coords에 저장된 리스트 불러오기\n",
    "country_list      = coords[\"country\"]       # Level-3 국가 이름 리스트\n",
    "super_list        = coords[\"super_region\"]  # Level-1 슈퍼리전\n",
    "region_list       = coords[\"region\"]        # Level-2 리전\n",
    "age_list          = coords[\"age\"]           # [\"Children\",\"Adult\",\"Both\"]\n",
    "meth_list         = coords[\"meth\"]          # [\"Interview based\",\"Non-interview\"]\n",
    "prev_list         = coords[\"prev\"]          # [\"point prevalence\",\"12-month prevalence\",\"life-time prevalence\"]\n",
    "\n",
    "# 3) 관심 범주의 posterior 샘플 꺼내기\n",
    "β0_samps         = post[\"β0\"].values                             # (samples,)\n",
    "β_age_Both       = post[\"β_age\"].sel(age=\"Both\").values          # (samples,)\n",
    "β_meth_IB        = post[\"β_meth\"].sel(meth=\"Interview based\").values  # (samples,)\n",
    "β_prev_life      = post[\"β_prev\"].sel(prev=\"life-time prevalence\").values  # (samples,)\n",
    "\n",
    "u_sr_samps       = post[\"u_sr\"].values      # (super_region, samples)\n",
    "u_r_samps        = post[\"u_r\"].values       # (region, samples)\n",
    "u_c_samps        = post[\"u_c\"].values       # (country, samples)\n",
    "\n",
    "# 4) country → region, super_region 매핑 (levels 파일에서 미리 만들어 두었다면 그것 사용)\n",
    "#    여기선 hier 데이터프레임에 있는 Level, Location Name, Parent ID 이용\n",
    "mapping = hier[[\"Level\",\"Location ID\",\"Location Name\",\"Parent ID\"]]\n",
    "# Level3\n",
    "lvl3 = mapping[mapping[\"Level\"]==3].rename(columns={\n",
    "    \"Location Name\":\"country\",\"Location ID\":\"loc3\",\"Parent ID\":\"parent2\"\n",
    "})\n",
    "# Level2\n",
    "lvl2 = mapping[mapping[\"Level\"]==2].rename(columns={\n",
    "    \"Location Name\":\"region\",\"Location ID\":\"loc2\",\"Parent ID\":\"parent1\"\n",
    "})\n",
    "# Level1\n",
    "lvl1 = mapping[mapping[\"Level\"]==1].rename(columns={\n",
    "    \"Location Name\":\"super_region\",\"Location ID\":\"loc1\"\n",
    "})\n",
    "\n",
    "# merge to get each country’s region & super_region names\n",
    "df_map = (\n",
    "    lvl3[[\"country\",\"loc3\",\"parent2\"]]\n",
    "    .merge(lvl2[[\"loc2\",\"region\",\"parent1\"]], left_on=\"parent2\", right_on=\"loc2\", how=\"left\")\n",
    "    .merge(lvl1[[\"loc1\",\"super_region\"]], left_on=\"parent1\", right_on=\"loc1\", how=\"left\")\n",
    "    [[\"country\",\"region\",\"super_region\"]]\n",
    ")\n",
    "\n",
    "# 5) Monte Carlo 샘플마다 μ 계산 → p = exp(μ)\n",
    "results = []\n",
    "n_samps = β0_samps.shape[0]\n",
    "for idx, row in df_map.iterrows():\n",
    "    country    = row[\"country\"]\n",
    "    region     = row[\"region\"]\n",
    "    super_reg  = row[\"super_region\"]\n",
    "    i_sr = super_list.index(super_reg)\n",
    "    i_r  = region_list.index(region)\n",
    "    i_c  = country_list.index(country)\n",
    "\n",
    "    # μ 샘플 벡터\n",
    "    mu = (\n",
    "        β0_samps\n",
    "        + β_age_Both\n",
    "        + β_meth_IB\n",
    "        + β_prev_life\n",
    "        + u_sr_samps[i_sr, :]\n",
    "        + u_r_samps[i_r, :]\n",
    "        + u_c_samps[i_c, :]\n",
    "    )\n",
    "    # prevalence 비율로 변환\n",
    "    p = np.exp(mu)\n",
    "\n",
    "    # median & 95% HDI\n",
    "    med = np.median(p)\n",
    "    hdi_lo, hdi_hi = az.hdi(p, hdi_prob=0.95)\n",
    "\n",
    "    results.append({\n",
    "        \"country\":             country,\n",
    "        \"region\":              region,\n",
    "        \"super_region\":        super_reg,\n",
    "        \"median_life_prev\":    med,\n",
    "        \"hdi_2.5%\":            hdi_lo,\n",
    "        \"hdi_97.5%\":           hdi_hi\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(results)\n",
    "\n",
    "# 6) 엑셀로 저장\n",
    "output_path = \"C:/Users/정이든/Desktop/연구실/9. prevalence of OCD/3차 와꾸/life_time_prevalence.xlsx\"\n",
    "df_out.to_excel(output_path, index=False)\n",
    "print(f\"Saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
